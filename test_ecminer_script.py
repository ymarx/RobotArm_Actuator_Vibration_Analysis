"""
ECMiner Stage1 ìŠ¤í¬ë¦½íŠ¸ ê²€ì¦ í…ŒìŠ¤íŠ¸
===================================
ëª©ì :
1. ecminer_stage1_feature_extraction.py ìŠ¤í¬ë¦½íŠ¸ ê²€ì¦
2. Raw CSV â†’ ìœˆë„ìš° Feature Table ë³€í™˜ í™•ì¸
3. ì¶”ì¶œëœ íŠ¹ì„± í’ˆì§ˆ ê²€ì¦
4. XGBoost ëª¨ë¸ í•™ìŠµ/ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))

print("=" * 80)
print("ECMiner Stage1 ìŠ¤í¬ë¦½íŠ¸ ê²€ì¦ í…ŒìŠ¤íŠ¸")
print("=" * 80)

# ==============================================================================
# 1. í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± (file_masterì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ)
# ==============================================================================
print("\n[1] í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì¤‘...")

# file_master ë¡œë“œ
file_master = pd.read_parquet(PROJECT_ROOT / "data" / "interim" / "file_master_v1.parquet")

# Normalê³¼ Abnormal ê°ê° ìƒ˜í”Œë§
normal_files = file_master[file_master['is_normal'] == True].copy()
abnormal_files = file_master[file_master['is_normal'] == False].copy()

# ìƒ˜í”Œ ì„ íƒ
test_files = []

# Normal: 200W_S03_CCW_R4 (time_split - will be split into train/test during window generation)
normal_file = normal_files[normal_files['file_id'] == '200W_S03_CCW_R4'].head(1)
test_files.append(normal_file)

# Abnormal Train: 1ê°œ
abnormal_train = abnormal_files[abnormal_files['split_set'] == 'train'].sample(1, random_state=42)
test_files.append(abnormal_train)

# Abnormal Test: 1ê°œ
abnormal_test = abnormal_files[abnormal_files['split_set'] == 'test'].sample(1, random_state=42)
test_files.append(abnormal_test)

# ë³‘í•©
test_file_list = pd.concat(test_files, ignore_index=True)

# ecmData í˜•ì‹ìœ¼ë¡œ ë³€í™˜
# IMPORTANT: For time_split files, we pass 'time_split' to let the script handle splitting
ecmData = pd.DataFrame({
    'file_path': test_file_list['file_path'],
    'label': test_file_list['label_binary'],
    'dataset_type': test_file_list['split_set'],  # Keep original split_set
    'file_id': test_file_list['file_id']
})

print(f"\ní…ŒìŠ¤íŠ¸ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (ì´ {len(ecmData)}ê°œ):")
print(ecmData.to_string(index=False))

# CSVë¡œ ì €ì¥
test_file_list_path = PROJECT_ROOT / "file_list_test.csv"
ecmData.to_csv(test_file_list_path, index=False)
print(f"\nì €ì¥ ì™„ë£Œ: {test_file_list_path}")

# ==============================================================================
# 2. Stage1 ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ecminer_stage1_feature_extraction.py)
# ==============================================================================
print("\n[2] Stage1 ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘...")

# ìŠ¤í¬ë¦½íŠ¸ import (ëª¨ë“ˆë¡œ ë¡œë“œ)
from ecminer_stage1_feature_extraction import build_feature_table_from_ecmdata

# ì‹¤í–‰
try:
    feature_table = build_feature_table_from_ecmdata(ecmData)
    print(f"\nâœ… Feature Table ìƒì„± ì™„ë£Œ!")
    print(f"  - ì´ ìœˆë„ìš°: {len(feature_table)}")
    print(f"  - ì»¬ëŸ¼ ê°œìˆ˜: {len(feature_table.columns)}")
    print(f"  - íŠ¹ì„± ì»¬ëŸ¼: {len([c for c in feature_table.columns if c not in ['file_id', 'window_idx', 'start_idx', 'end_idx', 'label', 'dataset_type', 'product', 'serial', 'condition', 'load']])}")

except Exception as e:
    print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ==============================================================================
# 3. ì¶”ì¶œëœ íŠ¹ì„± í’ˆì§ˆ ê²€ì¦
# ==============================================================================
print("\n[3] ì¶”ì¶œëœ íŠ¹ì„± í’ˆì§ˆ ê²€ì¦ ì¤‘...")

# ì»¬ëŸ¼ í™•ì¸
expected_features = [
    'acc_Y_rms', 'acc_X_rms', 'Gyro_Y_rms', 'Gyro_X_rms',
    'acc_Y_peak', 'acc_Sum_peak',
    'acc_Y_crest',
    'acc_Y_kurtosis', 'acc_Sum_kurtosis',
    'acc_Y_rms_low', 'acc_Y_rms_mid', 'acc_Y_rms_high',
    'acc_Sum_rms_low', 'acc_Sum_rms_mid', 'acc_Sum_rms_high',
    'Gyro_Y_rms_low', 'Gyro_Y_rms_mid', 'Gyro_Y_rms_high'
]

print("\níŠ¹ì„± ì¡´ì¬ ì—¬ë¶€:")
for feat in expected_features:
    exists = feat in feature_table.columns
    symbol = "âœ…" if exists else "âŒ"
    print(f"  {symbol} {feat}")

# NaN í™•ì¸
print("\n\nNaN ê°œìˆ˜:")
nan_counts = feature_table[expected_features].isna().sum()
for feat, count in nan_counts.items():
    if count > 0:
        print(f"  âš ï¸ {feat}: {count}ê°œ")

# ê¸°ë³¸ í†µê³„
print("\n\nê¸°ë³¸ í†µê³„ (Train ë°ì´í„°):")
train_data = feature_table[feature_table['dataset_type'] == 'train']
if len(train_data) > 0:
    stats = train_data[expected_features].describe()
    print(stats.T[['mean', 'std', 'min', 'max']].to_string())

# Label ë¶„í¬
print("\n\nLabel ë¶„í¬:")
label_dist = feature_table.groupby(['dataset_type', 'label']).size()
print(label_dist)

# ì €ì¥
output_path = PROJECT_ROOT / "feature_table_test.csv"
feature_table.to_csv(output_path, index=False)
print(f"\nì €ì¥ ì™„ë£Œ: {output_path}")

# ==============================================================================
# 4. XGBoost ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
# ==============================================================================
print("\n[4] XGBoost ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì¤‘...")

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    import xgboost as xgb
    from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix
except ImportError as e:
    print(f"âš ï¸ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ: {e}")
    print("  pip install xgboost scikit-learn")
    sys.exit(1)

# íŠ¹ì„± ì„ íƒ (XGBoost v3)
features_v3 = [
    'acc_Y_rms', 'acc_X_rms', 'Gyro_Y_rms', 'Gyro_X_rms',
    'acc_Y_peak', 'acc_Sum_peak',
    'acc_Y_crest',
    'acc_Y_kurtosis', 'acc_Sum_kurtosis',
    'acc_Y_rms_high', 'Gyro_Y_rms_high', 'Gyro_Y_rms_low'
]

# Train/Test ë¶„í• 
train_df = feature_table[feature_table['dataset_type'] == 'train'].copy()
test_df = feature_table[feature_table['dataset_type'] == 'test'].copy()

# NaN ì œê±°
train_df = train_df.dropna(subset=features_v3)
test_df = test_df.dropna(subset=features_v3)

print(f"\nTrain ì„¸íŠ¸: {len(train_df)} ìœˆë„ìš° (Normal: {(train_df['label']==1).sum()}, Abnormal: {(train_df['label']==0).sum()})")
print(f"Test ì„¸íŠ¸: {len(test_df)} ìœˆë„ìš° (Normal: {(test_df['label']==1).sum()}, Abnormal: {(test_df['label']==0).sum()})")

if len(train_df) == 0 or len(test_df) == 0:
    print("\nâš ï¸ Train ë˜ëŠ” Test ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    sys.exit(0)

# ë°ì´í„° ì¤€ë¹„
X_train = train_df[features_v3]
y_train = train_df['label']

X_test = test_df[features_v3]
y_test = test_df['label']

# XGBoost íŒŒë¼ë¯¸í„° (í”„ë¡œì íŠ¸ ê²€ì¦ê°’)
params = {
    'max_depth': 3,
    'min_child_weight': 5,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_lambda': 5,
    'reg_alpha': 1,
    'random_state': 42,
    'eval_metric': 'logloss',
    'scale_pos_weight': len(y_train[y_train==0]) / len(y_train[y_train==1]) if len(y_train[y_train==1]) > 0 else 1.0
}

print(f"\nXGBoost íŒŒë¼ë¯¸í„°:")
for key, value in params.items():
    print(f"  {key}: {value}")

# í•™ìŠµ
print("\nëª¨ë¸ í•™ìŠµ ì¤‘...")
model = xgb.XGBClassifier(**params)
model.fit(X_train, y_train, verbose=False)

print("âœ… í•™ìŠµ ì™„ë£Œ!")

# ì˜ˆì¸¡
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]  # Abnormal (class 0) í™•ë¥ 

# í‰ê°€
accuracy = accuracy_score(y_test, y_pred)

# AUC ê³„ì‚° (í´ë˜ìŠ¤ê°€ 2ê°œ ì´ìƒ ìˆì–´ì•¼ í•¨)
if len(np.unique(y_test)) > 1:
    # y_testê°€ {0, 1}ì´ê³  y_pred_probaëŠ” class 1ì˜ í™•ë¥ 
    # AUCë¥¼ ìœ„í•´ì„œëŠ” positive class (1)ì˜ í™•ë¥  ì‚¬ìš©
    auc = roc_auc_score(y_test, y_pred_proba)
else:
    auc = np.nan

print("\n" + "=" * 80)
print("ëª¨ë¸ í‰ê°€ ê²°ê³¼")
print("=" * 80)
print(f"\nAccuracy: {accuracy:.3f}")
if not np.isnan(auc):
    print(f"AUC: {auc:.3f}")
else:
    print("AUC: N/A (Test ì„¸íŠ¸ì— ë‹¨ì¼ í´ë˜ìŠ¤ë§Œ ì¡´ì¬)")

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

print("\nClassification Report:")
# Handle single-class case
unique_labels = np.unique(y_test)
if len(unique_labels) == 1:
    print(f"âš ï¸ Warning: Test set contains only class {unique_labels[0]}")
    if unique_labels[0] == 0:
        print(classification_report(y_test, y_pred, target_names=['Abnormal'], zero_division=0))
    else:
        print(classification_report(y_test, y_pred, target_names=['Normal'], zero_division=0))
else:
    print(classification_report(y_test, y_pred, target_names=['Abnormal', 'Normal'], zero_division=0))

# íŠ¹ì„± ì¤‘ìš”ë„
print("\níŠ¹ì„± ì¤‘ìš”ë„ (Top 5):")
feature_importance = pd.DataFrame({
    'feature': features_v3,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(feature_importance.head().to_string(index=False))

# ==============================================================================
# 5. ìµœì¢… ìš”ì•½
# ==============================================================================
print("\n" + "=" * 80)
print("ê²€ì¦ ìš”ì•½")
print("=" * 80)

print(f"\nâœ… Stage1 ìŠ¤í¬ë¦½íŠ¸ ê²€ì¦ ì™„ë£Œ!")
print(f"  - ì…ë ¥ íŒŒì¼: {len(ecmData)}ê°œ")
print(f"  - ì¶œë ¥ ìœˆë„ìš°: {len(feature_table)}ê°œ")
print(f"  - ì¶”ì¶œ íŠ¹ì„±: 18ê°œ")
print(f"  - NaN ë¹„ìœ¨: {feature_table[expected_features].isna().sum().sum() / (len(feature_table) * len(expected_features)) * 100:.1f}%")

print(f"\nâœ… XGBoost ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
print(f"  - Train: {len(train_df)} ìœˆë„ìš°")
print(f"  - Test: {len(test_df)} ìœˆë„ìš°")
print(f"  - Accuracy: {accuracy:.3f}")
if not np.isnan(auc):
    print(f"  - AUC: {auc:.3f}")

print(f"\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
print(f"  1. ì „ì²´ ë°ì´í„°ë¡œ ì¬ì‹¤í–‰ (file_list í™•ì¥)")
print(f"  2. ECMinerì— í†µí•© (íŒŒì´ì¬ ì—°ë™ ë…¸ë“œ)")
print(f"  3. Stage2/3 ë…¸ë“œ êµ¬ì„± (Filter, XGBoost)")

print("\n" + "=" * 80)
