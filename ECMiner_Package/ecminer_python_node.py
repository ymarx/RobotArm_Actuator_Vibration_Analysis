#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ECMiner íŒŒì´ì¬ ì—°ë™ ë…¸ë“œìš© ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš© ë°©ë²•:
1. ECMinerì—ì„œ ë¹ˆ ë…¸ë“œ â†’ Python ì—°ë™ ë…¸ë“œ ì¶”ê°€
2. ì´ ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš©ì„ í¸ì§‘ê¸°ì— ë³µì‚¬-ë¶™ì—¬ë„£ê¸°
3. ì•„ë˜ "ê²½ë¡œ ì„¤ì •" ì„¹ì…˜ì˜ PROJECT_ROOTë§Œ ìˆ˜ì •
4. ì‹¤í–‰ â†’ ë‹¤ìŒ ë…¸ë“œë¡œ ê²°ê³¼ ì „ë‹¬ë¨

ì£¼ì˜ì‚¬í•­:
- ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ECMiner ì„ì‹œ í´ë”ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤
- ëª¨ë“  íŒŒì¼ ì°¸ì¡°ëŠ” ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
- ìƒëŒ€ ê²½ë¡œ(SCRIPT_DIR ë“±)ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
"""

# ============================================================================
# ğŸ”§ ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ìê°€ ìˆ˜ì •í•˜ëŠ” ì˜ì—­)
# ============================================================================
#
# ECMiner íŒŒì´ì¬ ì—°ë™ ë…¸ë“œë¥¼ ë§Œë“¤ ë•Œë§ˆë‹¤ ì•„ë˜ ê²½ë¡œë¥¼ ì‹œìŠ¤í…œì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”
#
# ============================================================================

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ)
# ì´ ê²½ë¡œ ì•„ë˜ì— data/, ECMiner_Package/ í´ë”ê°€ ìˆì–´ì•¼ í•¨
#
# Windows ì˜ˆì‹œ: "C:/Users/UserName/Projects/RobotArm"
# macOS ì˜ˆì‹œ:   "/Users/UserName/Dropbox/RobotArm"
# Linux ì˜ˆì‹œ:   "/home/username/projects/robotarm"

PROJECT_ROOT = "/Users/YMARX/Dropbox/2025_ECMiner/CP25_NeuroMecha/03_ì§„í–‰/[Analysis]RobotArm_Actuator_QT(Ociliation)"

# ì¶œë ¥ CSV íŒŒì¼ ì €ì¥ ì—¬ë¶€ (True/False)
# True: íŒŒì¼ë¡œ ì €ì¥ (ë””ë²„ê¹…/í™•ì¸ìš©)
# False: ECMiner ë‹¤ìŒ ë…¸ë“œë¡œë§Œ ì „ë‹¬
SAVE_OUTPUT_FILE = True

# ì¶œë ¥ íŒŒì¼ëª… (SAVE_OUTPUT_FILE=True ì¸ ê²½ìš°ë§Œ ì‚¬ìš©)
OUTPUT_FILENAME = "ecminer_output.csv"

# ============================================================================
# âš ï¸ ì´ ì•„ë˜ ì½”ë“œëŠ” ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”
# ============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from scipy import signal
from typing import Dict, List, Tuple, Optional
import warnings
import yaml
import re
from datetime import datetime

warnings.filterwarnings('ignore')

# ì ˆëŒ€ ê²½ë¡œ êµ¬ì„±
PROJECT_ROOT = Path(PROJECT_ROOT)
DATA_ROOT = PROJECT_ROOT / "data"
LABEL_CSV_PATH = PROJECT_ROOT / "ECMiner_Package" / "ecminer_labels.csv"
CONFIG_YAML_PATH = PROJECT_ROOT / "ECMiner_Package" / "ecminer_config.yaml"
OUTPUT_CSV_PATH = PROJECT_ROOT / OUTPUT_FILENAME if SAVE_OUTPUT_FILE else None

print("=" * 60)
print("ECMiner íŒŒì´ì¬ ì—°ë™ ë…¸ë“œ: ì§„ë™ ë°ì´í„° ì „ì²˜ë¦¬")
print("=" * 60)
print(f"\nPROJECT_ROOT: {PROJECT_ROOT}")
print(f"DATA_ROOT: {DATA_ROOT}")
print(f"LABEL_CSV: {LABEL_CSV_PATH}")
print(f"CONFIG_YAML: {CONFIG_YAML_PATH}")

# ============================================================================
# 1. ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ============================================================================

# ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ (Hz)
SAMPLING_FREQ = 512.0

# ìœˆë„ìš° íŒŒë¼ë¯¸í„°
WINDOW_SEC = 8.0      # 8ì´ˆ ìœˆë„ìš°
HOP_SEC = 4.0         # 4ì´ˆ hop (50% ì¤‘ì²©)
STABLE_MARGIN = 0.1   # ì•/ë’¤ 10% ì•ˆì • êµ¬ê°„ ì œì™¸

# ì–‘í’ˆ íŒŒì¼ ì •ì˜ (100W: Sample00ë§Œ, 200W: Sample03ë§Œ)
NORMAL_SAMPLES = {
    '100W': [0],
    '200W': [3]
}

# ë°ì´í„°ì…‹ ë¶„í•  ë¹„ìœ¨
TIME_SPLIT_RATIOS = {
    'train': 0.6,
    'val': 0.2,
    'test': 0.2
}

ABNORMAL_SPLIT_RATIOS = {
    'train': 0.7,
    'val': 0.15,
    'test': 0.15
}

RANDOM_SEED = 42

# íŠ¹ì§• ì´ë¦„ ì •ì˜
BASIC_FEATURES = [
    'acc_Y_rms', 'acc_Y_peak', 'acc_Y_crest',
    'acc_Sum_rms', 'acc_Sum_peak', 'acc_Sum_crest',
    'Gyro_Y_rms', 'Gyro_Y_peak', 'Gyro_Y_crest'
]

BAND_RMS_FEATURES = [
    'acc_Y_rms_low', 'acc_Y_rms_mid', 'acc_Y_rms_high',
    'acc_Sum_rms_low', 'acc_Sum_rms_mid', 'acc_Sum_rms_high',
    'Gyro_Y_rms_low', 'Gyro_Y_rms_mid', 'Gyro_Y_rms_high'
]

# ì£¼íŒŒìˆ˜ ë°´ë“œ ì •ì˜ (Hz)
FREQ_BANDS = {
    'low': (0, 50),
    'mid': (50, 150),
    'high': (150, 256)  # Nyquist ì£¼íŒŒìˆ˜ì˜ ì ˆë°˜
}

# ============================================================================
# 2. ë ˆì´ë¸” ì‹œìŠ¤í…œ í•¨ìˆ˜
# ============================================================================

def load_label_file(label_path: Path) -> pd.DataFrame:
    """ë ˆì´ë¸” íŒŒì¼ ë¡œë“œ (CSV ë˜ëŠ” Excel)"""
    if label_path.suffix == '.csv':
        return pd.read_csv(label_path, encoding='utf-8-sig')
    elif label_path.suffix in ['.xlsx', '.xls']:
        return pd.read_excel(label_path)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {label_path.suffix}")

def load_config(config_path: Path) -> Dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ (YAML)"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def apply_label_strategy(
    label_raw: str,
    product: str,
    sample: int,
    strategy: Dict
) -> Tuple[int, float]:
    """ë ˆì´ë¸” ì „ëµ ì ìš©: label_raw â†’ (label_binary, label_weight)"""
    # 1. Special overrides ë¨¼ì € ì ìš©
    original_label = label_raw
    for override in strategy.get('special_overrides', []):
        if (product == override['product'] and
            sample == override['sample'] and
            label_raw == override['from']):
            label_raw = override['to']
            break

    # 2. PASS/FAIL ë§¤í•‘
    if label_raw in strategy['PASS']:
        label_binary = 1  # ì–‘í’ˆ
    elif label_raw in strategy['FAIL']:
        label_binary = 0  # ë¶ˆëŸ‰
    else:
        label_binary = 0

    # 3. ê°€ì¤‘ì¹˜
    label_weight = strategy.get('weights', {}).get(original_label, 1.0)

    return label_binary, label_weight

# ============================================================================
# 3. ë°ì´í„° ë¡œë“œ ë° íŒŒì‹± í•¨ìˆ˜
# ============================================================================

def parse_file_id(file_id: str) -> Dict:
    """
    íŒŒì¼ IDë¥¼ íŒŒì‹±í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

    ì…ë ¥: "100W_Sample00_CW_20251107_034124" (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    ì¶œë ¥: {'product': '100W', 'sample': 0, 'direction': 'CW'}
    """
    parts = file_id.split('_')
    product = parts[0]
    sample_str = parts[1].replace('Sample', '')
    sample = int(sample_str)
    direction = parts[2]

    return {
        'product': product,
        'sample': sample,
        'direction': direction
    }

def parse_filename(filename: str, product: str) -> Optional[Dict]:
    """
    íŒŒì¼ëª…ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

    ì˜ˆ: "100W_Sample00 cw4_2025-11-07 03-41-24.csv"
    â†’ product=100W, sample=0, direction=CW, timestamp=20251107_034124
    """
    # Sample ë²ˆí˜¸ ì¶”ì¶œ
    sample_match = re.search(r'Sample(\d+)', filename, re.IGNORECASE)
    if not sample_match:
        return None

    sample = int(sample_match.group(1))

    # íšŒì „ ë°©í–¥ ì¶”ì¶œ (cw/ccw)
    direction_match = re.search(r'(cw|ccw)', filename, re.IGNORECASE)
    if not direction_match:
        return None

    direction = direction_match.group(1).upper()

    # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
    timestamp_match = re.search(r'(\d{4})-(\d{2})-(\d{2})\s+(\d{2})-(\d{2})-(\d{2})', filename)
    if timestamp_match:
        y, m, d, h, mi, s = timestamp_match.groups()
        timestamp = f"{y}{m}{d}_{h}{mi}{s}"
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    return {
        'product': product,
        'sample': sample,
        'direction': direction,
        'timestamp': timestamp
    }

def is_normal_file(product: str, sample: int) -> bool:
    """ì–‘í’ˆ íŒŒì¼ ì—¬ë¶€ íŒë‹¨"""
    return sample in NORMAL_SAMPLES.get(product, [])

def load_csv_file(file_path: Path) -> pd.DataFrame:
    """
    CSV íŒŒì¼ ë¡œë“œ (7ê°œ ì±„ë„)
    ë©”íƒ€ë°ì´í„°ë¥¼ ê±´ë„ˆë›°ê³  ì‹¤ì œ ë°ì´í„°ë§Œ ì½ê¸°
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # "DataSet" ì¤„ ì°¾ê¸°
    data_start_idx = None
    for i, line in enumerate(lines):
        if line.strip() == 'DataSet':
            data_start_idx = i + 1
            break

    if data_start_idx is None:
        raise ValueError(f"'DataSet' ì¤„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

    # í—¤ë” + ë°ì´í„° ì½ê¸°
    df = pd.read_csv(file_path, skiprows=data_start_idx)

    # TimeStamp ì»¬ëŸ¼ ì œê±°
    if 'TimeStamp' in df.columns:
        df = df.drop(columns=['TimeStamp'])

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = df.interpolate(method='linear', axis=0).fillna(method='bfill').fillna(method='ffill')

    return df

# ============================================================================
# 4. ìœˆë„ìš° ì„¸ê·¸ë¨¼í…Œì´ì…˜ í•¨ìˆ˜
# ============================================================================

def get_stable_range(total_length: int, margin: float = 0.1) -> Tuple[int, int]:
    """ì•ˆì • êµ¬ê°„ ì¶”ì¶œ (ì•/ë’¤ margin% ì œì™¸)"""
    margin_samples = int(total_length * margin)
    start = margin_samples
    end = total_length - margin_samples
    return start, end

def create_windows(data: pd.DataFrame, fs: float, window_sec: float, hop_sec: float) -> List[np.ndarray]:
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„± (ì•ˆì • êµ¬ê°„ë§Œ ì‚¬ìš©)"""
    stable_start, stable_end = get_stable_range(len(data), STABLE_MARGIN)
    stable_data = data.iloc[stable_start:stable_end]

    window_samples = int(window_sec * fs)
    hop_samples = int(hop_sec * fs)

    windows = []
    start = 0
    while start + window_samples <= len(stable_data):
        window = stable_data.iloc[start:start+window_samples].values
        windows.append(window)
        start += hop_samples

    return windows

def assign_time_split(window_idx: int, total_windows: int) -> str:
    """ì‹œê°„ ìˆœì„œëŒ€ë¡œ train/val/test ë¶„í• """
    train_end = int(total_windows * TIME_SPLIT_RATIOS['train'])
    val_end = train_end + int(total_windows * TIME_SPLIT_RATIOS['val'])

    if window_idx < train_end:
        return 'train'
    elif window_idx < val_end:
        return 'val'
    else:
        return 'test'

# ============================================================================
# 5. íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
# ============================================================================

def extract_window_features(window: np.ndarray, fs: float) -> Dict:
    """ìœˆë„ìš°ì—ì„œ 18ê°œ íŠ¹ì§• ì¶”ì¶œ"""
    features = {}

    # ì±„ë„ ì„ íƒ (acc-Y, acc-Sum, Gyro-Y)
    channels = {
        'acc_Y': 1,
        'acc_Sum': 3,
        'Gyro_Y': 5
    }

    for ch_name, ch_idx in channels.items():
        signal_data = window[:, ch_idx]

        # ê¸°ë³¸ í†µê³„ëŸ‰
        features[f'{ch_name}_rms'] = np.sqrt(np.mean(signal_data**2))
        features[f'{ch_name}_peak'] = np.max(np.abs(signal_data))
        features[f'{ch_name}_crest'] = features[f'{ch_name}_peak'] / (features[f'{ch_name}_rms'] + 1e-10)

        # ë°´ë“œë³„ RMS
        for band_name, (f_low, f_high) in FREQ_BANDS.items():
            sos = signal.butter(4, [f_low, f_high], btype='band', fs=fs, output='sos')
            filtered = signal.sosfilt(sos, signal_data)
            features[f'{ch_name}_rms_{band_name}'] = np.sqrt(np.mean(filtered**2))

    return features

# ============================================================================
# 6. ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================================================

def process_file(
    file_path: Path,
    file_id: str,
    fs: float,
    label_raw: str,
    label_binary: int,
    label_weight: float
) -> List[Dict]:
    """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬: CSV ë¡œë“œ â†’ ìœˆë„ìš° ìƒì„± â†’ íŠ¹ì§• ì¶”ì¶œ"""
    # ë©”íƒ€ë°ì´í„° íŒŒì‹±
    meta = parse_file_id(file_id)
    product = meta['product']
    sample = meta['sample']
    direction = meta['direction']

    # CSV íŒŒì¼ ë¡œë“œ
    timeseries = load_csv_file(file_path)

    # ìœˆë„ìš° ìƒì„±
    windows = create_windows(timeseries, fs, WINDOW_SEC, HOP_SEC)

    # ê° ìœˆë„ìš° ì²˜ë¦¬
    window_features_list = []

    for window_idx, window in enumerate(windows):
        # ë°ì´í„°ì…‹ í• ë‹¹
        if is_normal_file(product, sample):
            # ì–‘í’ˆ: ì‹œê°„ ê¸°ë°˜ ë¶„í• 
            dataset_type = assign_time_split(window_idx, len(windows))
        else:
            # ë¶ˆëŸ‰: trainìœ¼ë¡œ ê¸°ë³¸ ì„¤ì • (ë‚˜ì¤‘ì— íŒŒì¼ ë‹¨ìœ„ë¡œ ì¬ë¶„í• )
            dataset_type = 'train'

        # íŠ¹ì§• ì¶”ì¶œ
        features = extract_window_features(window, fs)

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        window_data = {
            'window_id': f"{file_id}_win{window_idx:03d}",
            'file_id': file_id,
            'dataset_type': dataset_type,
            'label_raw': label_raw,
            'label_binary': label_binary,
            'label_weight': label_weight,
            'product': product,
            'sample': sample,
            'direction': direction
        }
        window_data.update(features)

        window_features_list.append(window_data)

    return window_features_list

def assign_abnormal_splits(df: pd.DataFrame) -> pd.DataFrame:
    """ë¶ˆëŸ‰ íŒŒì¼ì˜ ìœˆë„ìš°ë¥¼ train/val/testë¡œ ë¶„í• """
    normal_df = df[df['label_binary'] == 1].copy()
    abnormal_df = df[df['label_binary'] == 0].copy()

    if len(abnormal_df) == 0:
        return df

    # íŒŒì¼ IDë³„ë¡œ ê·¸ë£¹í™”
    file_ids = abnormal_df['file_id'].unique()
    np.random.seed(RANDOM_SEED)
    np.random.shuffle(file_ids)

    # íŒŒì¼ ë‹¨ìœ„ë¡œ train/val/test ë¶„í• 
    n_files = len(file_ids)
    n_train = int(n_files * ABNORMAL_SPLIT_RATIOS['train'])
    n_val = int(n_files * ABNORMAL_SPLIT_RATIOS['val'])

    train_files = file_ids[:n_train]
    val_files = file_ids[n_train:n_train+n_val]
    test_files = file_ids[n_train+n_val:]

    # íŒŒì¼ IDì— ë”°ë¼ dataset_type í• ë‹¹
    abnormal_df.loc[abnormal_df['file_id'].isin(train_files), 'dataset_type'] = 'train'
    abnormal_df.loc[abnormal_df['file_id'].isin(val_files), 'dataset_type'] = 'val'
    abnormal_df.loc[abnormal_df['file_id'].isin(test_files), 'dataset_type'] = 'test'

    # ì–‘í’ˆê³¼ ë¶ˆëŸ‰ í•©ì¹˜ê¸°
    result_df = pd.concat([normal_df, abnormal_df], ignore_index=True)

    return result_df

def scan_data_folder(data_root: Path, product: str) -> List[Dict]:
    """
    ë°ì´í„° í´ë”ë¥¼ ìŠ¤ìº”í•˜ì—¬ íŒŒì¼ ëª©ë¡ ìƒì„±

    Args:
        data_root: data í´ë” ê²½ë¡œ
        product: "100W" ë˜ëŠ” "200W"

    Returns:
        íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{'file_path': Path, 'file_id': str}, ...]
    """
    product_folder = data_root / product

    if not product_folder.exists():
        print(f"ê²½ê³ : {product_folder} í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    files = []
    csv_files = list(product_folder.glob("*.csv"))

    for csv_file in csv_files:
        filename = csv_file.name

        # íŒŒì¼ëª… íŒŒì‹±
        meta = parse_filename(filename, product)
        if meta is None:
            print(f"ê²½ê³ : íŒŒì¼ëª… íŒŒì‹± ì‹¤íŒ¨, ê±´ë„ˆëœ€: {filename}")
            continue

        # file_id ìƒì„±
        file_id = f"{meta['product']}_Sample{meta['sample']:02d}_{meta['direction']}_{meta['timestamp']}"

        files.append({
            'file_path': csv_file,
            'file_id': file_id
        })

    return files

# ============================================================================
# 7. ì‹¤í–‰ ì½”ë“œ
# ============================================================================

print("\n[1ë‹¨ê³„] ë°ì´í„° í´ë” ìŠ¤ìº”")
print("-" * 60)

# 100W, 200W í´ë” ìŠ¤ìº”
all_files = []
all_files.extend(scan_data_folder(DATA_ROOT, "100W"))
all_files.extend(scan_data_folder(DATA_ROOT, "200W"))

print(f"âœ“ ì´ {len(all_files)}ê°œ íŒŒì¼ ë°œê²¬")

# ============================================================================
# ë ˆì´ë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
# ============================================================================

print("\n[2ë‹¨ê³„] ë ˆì´ë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
print("-" * 60)

# ë ˆì´ë¸” íŒŒì¼ ë¡œë“œ
if not LABEL_CSV_PATH.exists():
    raise FileNotFoundError(f"ë ˆì´ë¸” íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LABEL_CSV_PATH}")

labels_df = load_label_file(LABEL_CSV_PATH)
print(f"âœ“ ë ˆì´ë¸” íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(labels_df)}ê°œ ë ˆì´ë¸”")

# ì„¤ì • íŒŒì¼ ë¡œë“œ
if not CONFIG_YAML_PATH.exists():
    raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CONFIG_YAML_PATH}")

config = load_config(CONFIG_YAML_PATH)
strategy_name = config['label_strategy']
strategy = config['strategies'][strategy_name]
print(f"âœ“ ë ˆì´ë¸” ì „ëµ: {strategy_name} - {strategy.get('description', '')}")

# ============================================================================
# ì „ì²´ íŒŒì¼ ì²˜ë¦¬
# ============================================================================

print("\n[3ë‹¨ê³„] íŒŒì¼ ì²˜ë¦¬")
print("-" * 60)

all_windows = []
processed_count = 0
error_count = 0

for file_info in all_files:
    try:
        file_path = file_info['file_path']
        file_id = file_info['file_id']

        # file_idì—ì„œ product, sample ì¶”ì¶œ
        meta = parse_file_id(file_id)
        product = meta['product']
        sample = meta['sample']

        # ë ˆì´ë¸” ë§¤ì¹­
        label_row = labels_df[
            (labels_df['product'] == product) &
            (labels_df['sample'] == sample)
        ]

        if len(label_row) == 0:
            print(f"ê²½ê³ : ë ˆì´ë¸” ì—†ìŒ, ê±´ë„ˆëœ€: {product} Sample{sample:02d}")
            error_count += 1
            continue

        label_raw = label_row['label'].iloc[0]

        # ë ˆì´ë¸” ì „ëµ ì ìš©
        label_binary, label_weight = apply_label_strategy(
            label_raw, product, sample, strategy
        )

        # íŒŒì¼ ì²˜ë¦¬
        windows = process_file(
            file_path, file_id, SAMPLING_FREQ,
            label_raw=label_raw,
            label_binary=label_binary,
            label_weight=label_weight
        )
        all_windows.extend(windows)
        processed_count += 1

        if processed_count % 10 == 0:
            print(f"  ì§„í–‰ì¤‘: {processed_count}/{len(all_files)} íŒŒì¼ ì²˜ë¦¬ë¨...")

    except Exception as e:
        print(f"ê²½ê³ : íŒŒì¼ {file_info['file_id']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        error_count += 1
        continue

print(f"âœ“ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")

# ë°ì´í„°í”„ë ˆì„ ìƒì„±
output_df = pd.DataFrame(all_windows)

# ë¶ˆëŸ‰ íŒŒì¼ ë°ì´í„°ì…‹ ë¶„í• 
output_df = assign_abnormal_splits(output_df)

# ê²°ê³¼ ì •ë ¬ (train â†’ val â†’ test ìˆœì„œ)
dataset_order = {'train': 0, 'val': 1, 'test': 2}
output_df['_sort_key'] = output_df['dataset_type'].map(dataset_order)
output_df = output_df.sort_values('_sort_key').drop(columns=['_sort_key']).reset_index(drop=True)

# ============================================================================
# ê²°ê³¼ ì¶œë ¥
# ============================================================================

print("\n[4ë‹¨ê³„] ì²˜ë¦¬ ì™„ë£Œ")
print("-" * 60)
print(f"  - ì´ ìœˆë„ìš° ìˆ˜: {len(output_df)}")
print(f"  - Train: {len(output_df[output_df['dataset_type']=='train'])}")
print(f"  - Val: {len(output_df[output_df['dataset_type']=='val'])}")
print(f"  - Test: {len(output_df[output_df['dataset_type']=='test'])}")
print(f"  - ì •ìƒ: {len(output_df[output_df['label_binary']==1])}")
print(f"  - ë¶ˆëŸ‰: {len(output_df[output_df['label_binary']==0])}")
print(f"  - íŠ¹ì§• ê°œìˆ˜: {len(BASIC_FEATURES + BAND_RMS_FEATURES)}ê°œ")

# NaN ê²€ì‚¬
if output_df.isnull().any().any():
    print("ê²½ê³ : ê²°ê³¼ì— NaN ê°’ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    nan_cols = output_df.columns[output_df.isnull().any()].tolist()
    print(f"  NaN í¬í•¨ ì»¬ëŸ¼: {nan_cols}")

# ì¤‘ë³µ window_id ê²€ì‚¬
if output_df['window_id'].duplicated().any():
    n_duplicates = output_df['window_id'].duplicated().sum()
    print(f"ê²½ê³ : {n_duplicates}ê°œì˜ ì¤‘ë³µ window_idê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("  âœ“ ì¤‘ë³µ window_id ì—†ìŒ")

# ECMinerê°€ ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬í•  ë°ì´í„°
ecmData = output_df

# íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒì‚¬í•­)
if OUTPUT_CSV_PATH is not None:
    output_df.to_csv(OUTPUT_CSV_PATH, index=False)
    print(f"\nâœ“ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {OUTPUT_CSV_PATH}")

print("\n" + "=" * 60)
print("ECMiner ì¶œë ¥ ì¤€ë¹„ ì™„ë£Œ!")
print("=" * 60)
