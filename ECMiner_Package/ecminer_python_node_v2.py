#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ECMiner íŒŒì´ì¬ ì—°ë™ ë…¸ë“œìš© ìŠ¤í¬ë¦½íŠ¸ v2 (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ecminer_stage1_py_node.pyì˜ ê²€ì¦ëœ ì…ë ¥ ë°©ì‹ê³¼
ì ˆëŒ€ ê²½ë¡œ ì‹œìŠ¤í…œì„ ê²°í•©í•œ ì•ˆì •ì ì¸ ë²„ì „ì…ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•:
1. create_ecminer_input.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ecminer_input_full.csv ìƒì„±
2. ECMinerì—ì„œ ë¹ˆ ë…¸ë“œ â†’ Python ì—°ë™ ë…¸ë“œ ì¶”ê°€
3. ì´ ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš©ì„ í¸ì§‘ê¸°ì— ë³µì‚¬-ë¶™ì—¬ë„£ê¸°
4. ì•„ë˜ "ê²½ë¡œ ì„¤ì •" ì„¹ì…˜ì˜ PROJECT_ROOTë§Œ ìˆ˜ì •
5. ì‹¤í–‰ â†’ ë‹¤ìŒ ë…¸ë“œë¡œ ê²°ê³¼ ì „ë‹¬ë¨

ì£¼ì˜ì‚¬í•­:
- ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ECMiner ì„ì‹œ í´ë”ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤
- ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì¸ íŒŒì¼ ì ‘ê·¼ì„ ë³´ì¥í•©ë‹ˆë‹¤
- ecminer_input_full.csvê°€ ë¯¸ë¦¬ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
"""

# ============================================================================
# ğŸ”§ ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ìê°€ ìˆ˜ì •í•˜ëŠ” ì˜ì—­)
# ============================================================================
#
# ECMiner íŒŒì´ì¬ ì—°ë™ ë…¸ë“œë¥¼ ë§Œë“¤ ë•Œë§ˆë‹¤ ì•„ë˜ ê²½ë¡œë¥¼ ì‹œìŠ¤í…œì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”
#
# ============================================================================

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ)
# ì´ ê²½ë¡œ ì•„ë˜ì— ë‹¤ìŒ íŒŒì¼/í´ë”ê°€ ìˆì–´ì•¼ í•¨:
#   - ecminer_input_full.csv (create_ecminer_input.pyë¡œ ìƒì„±)
#   - data/ (ì›ë³¸ ë°ì´í„° í´ë”)
#   - ECMiner_Package/ (ë ˆì´ë¸” íŒŒì¼ ë° ì„¤ì •)
#
# Windows ì˜ˆì‹œ: "C:/Users/UserName/Projects/RobotArm"
# macOS ì˜ˆì‹œ:   "/Users/UserName/Dropbox/RobotArm"
# Linux ì˜ˆì‹œ:   "/home/username/projects/robotarm"

PROJECT_ROOT = "/Users/YMARX/Dropbox/2025_ECMiner/CP25_NeuroMecha/03_ì§„í–‰/[Analysis]RobotArm_Actuator_QT(Ociliation)"

# ì¶œë ¥ CSV íŒŒì¼ ì €ì¥ ì—¬ë¶€ (True/False)
# True: íŒŒì¼ë¡œ ì €ì¥ (ë””ë²„ê¹…/í™•ì¸ìš©)
# False: ECMiner ë‹¤ìŒ ë…¸ë“œë¡œë§Œ ì „ë‹¬
SAVE_OUTPUT_FILE = True

# ì¶œë ¥ íŒŒì¼ëª… (SAVE_OUTPUT_FILE=True ì¸ ê²½ìš°ë§Œ ì‚¬ìš©)
OUTPUT_FILENAME = "ecminer_output.csv"

# ============================================================================
# âš ï¸ ì´ ì•„ë˜ ì½”ë“œëŠ” ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”
# ============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from scipy import signal
from typing import Dict, List, Tuple, Optional
import warnings
import yaml
warnings.filterwarnings('ignore')

# ì ˆëŒ€ ê²½ë¡œ êµ¬ì„±
PROJECT_ROOT = Path(PROJECT_ROOT)
INPUT_CSV_PATH = PROJECT_ROOT / "ecminer_input_full.csv"
DATA_ROOT = PROJECT_ROOT / "data"
LABEL_CSV_PATH = PROJECT_ROOT / "ECMiner_Package" / "ecminer_labels.csv"
CONFIG_YAML_PATH = PROJECT_ROOT / "ECMiner_Package" / "ecminer_config.yaml"
OUTPUT_CSV_PATH = PROJECT_ROOT / OUTPUT_FILENAME if SAVE_OUTPUT_FILE else None

print("=" * 60)
print("ECMiner íŒŒì´ì¬ ì—°ë™ ë…¸ë“œ v2: ì§„ë™ ë°ì´í„° ì „ì²˜ë¦¬")
print("=" * 60)
print(f"\nPROJECT_ROOT: {PROJECT_ROOT}")
print(f"INPUT_CSV: {INPUT_CSV_PATH}")
print(f"DATA_ROOT: {DATA_ROOT}")
print(f"LABEL_CSV: {LABEL_CSV_PATH}")
print(f"CONFIG_YAML: {CONFIG_YAML_PATH}")

# ============================================================================
# 1. ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ============================================================================

# ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ (Hz)
SAMPLING_FREQ = 512.0

# ìœˆë„ìš° íŒŒë¼ë¯¸í„°
WINDOW_SEC = 8.0      # 8ì´ˆ ìœˆë„ìš°
HOP_SEC = 4.0         # 4ì´ˆ hop (50% ì¤‘ì²©)
STABLE_MARGIN = 0.1   # ì•/ë’¤ 10% ì•ˆì • êµ¬ê°„ ì œì™¸

# ì–‘í’ˆ íŒŒì¼ ì •ì˜ (100W: Sample00ë§Œ, 200W: Sample03ë§Œ)
NORMAL_SAMPLES = {
    '100W': [0],
    '200W': [3]
}

# Train/Val/Test ì‹œê°„ ë¶„í•  ë¹„ìœ¨ (ì–‘í’ˆ íŒŒì¼ìš©)
TIME_SPLIT_RANGES = {
    'train': (0.0, 0.6),   # 0-60%
    'val': (0.6, 0.8),     # 60-80%
    'test': (0.8, 1.0)     # 80-100%
}

# ë¶ˆëŸ‰ íŒŒì¼ ë¶„í•  ë¹„ìœ¨
ABNORMAL_SPLIT_RATIOS = {
    'train': 0.7,
    'val': 0.15,
    'test': 0.15
}
RANDOM_SEED = 42

# ë°´ë“œ RMS ì£¼íŒŒìˆ˜ ë²”ìœ„ (Hz)
FREQUENCY_BANDS = {
    'low': (1, 10),      # ì €ì£¼íŒŒ: êµ¬ì¡° ì§„ë™
    'mid': (10, 50),     # ì¤‘ì£¼íŒŒ: íšŒì „ ê³ ì¡°íŒŒ
    'high': (50, 150)    # ê³ ì£¼íŒŒ: ë² ì–´ë§ ê²°í•¨, ì¶©ê²©
}

# ì¶”ì¶œí•  íŠ¹ì§• ëª©ë¡
BASIC_FEATURES = [
    'acc_Y_rms', 'acc_Y_peak', 'acc_Y_crest',
    'acc_Sum_rms', 'acc_Sum_peak', 'acc_Sum_crest',
    'Gyro_Y_rms', 'Gyro_Y_peak', 'Gyro_Y_crest'
]

BAND_RMS_FEATURES = [
    'acc_Y_rms_low', 'acc_Y_rms_mid', 'acc_Y_rms_high',
    'acc_Sum_rms_low', 'acc_Sum_rms_mid', 'acc_Sum_rms_high',
    'Gyro_Y_rms_low', 'Gyro_Y_rms_mid', 'Gyro_Y_rms_high'
]

# ============================================================================
# 2. ë ˆì´ë¸” ì‹œìŠ¤í…œ í•¨ìˆ˜
# ============================================================================

def load_label_file(label_path: Path) -> pd.DataFrame:
    """
    ë ˆì´ë¸” íŒŒì¼ ë¡œë“œ (CSV ë˜ëŠ” Excel)

    Returns:
        DataFrame with columns: product, sample, label
    """
    if label_path.suffix == '.csv':
        return pd.read_csv(label_path, encoding='utf-8-sig')
    elif label_path.suffix in ['.xlsx', '.xls']:
        return pd.read_excel(label_path)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {label_path.suffix}")

def load_config(config_path: Path) -> Dict:
    """
    ì„¤ì • íŒŒì¼ ë¡œë“œ (YAML)

    Returns:
        Config dictionary
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def apply_label_strategy(
    label_raw: str,
    product: str,
    sample: int,
    strategy: Dict
) -> Tuple[int, float]:
    """
    ë ˆì´ë¸” ì „ëµ ì ìš©: label_raw â†’ (label_binary, label_weight)

    Args:
        label_raw: ì›ë³¸ ë¼ë²¨ (ì •ìƒ/ì†ŒìŒ/ì§„ë™/í‘œê¸°ì—†ìŒ)
        product: ì œí’ˆ (100W/200W)
        sample: ìƒ˜í”Œ ë²ˆí˜¸
        strategy: ì „ëµ ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Returns:
        (label_binary, label_weight) - (1=ì–‘í’ˆ/0=ë¶ˆëŸ‰, ê°€ì¤‘ì¹˜)
    """
    # 1. Special overrides ë¨¼ì € ì ìš©
    original_label = label_raw
    for override in strategy.get('special_overrides', []):
        if (product == override['product'] and
            sample == override['sample'] and
            label_raw == override['from']):
            label_raw = override['to']
            break

    # 2. PASS/FAIL ë§¤í•‘
    if label_raw in strategy['PASS']:
        label_binary = 1  # ì–‘í’ˆ
    elif label_raw in strategy['FAIL']:
        label_binary = 0  # ë¶ˆëŸ‰
    else:
        # ì •ì˜ë˜ì§€ ì•Šì€ ë¼ë²¨ì€ ë¶ˆëŸ‰ìœ¼ë¡œ ì²˜ë¦¬
        label_binary = 0

    # 3. ê°€ì¤‘ì¹˜
    label_weight = strategy.get('weights', {}).get(original_label, 1.0)

    return label_binary, label_weight

# ============================================================================
# 3. ë°ì´í„° ë¡œë“œ ë° íŒŒì‹± í•¨ìˆ˜
# ============================================================================

def parse_file_id(file_id: str) -> Dict:
    """
    íŒŒì¼ IDë¥¼ íŒŒì‹±í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

    ì…ë ¥: "100W_Sample00_CW_20251107_034124" (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    ì¶œë ¥: {'product': '100W', 'sample': 0, 'direction': 'CW'}
    """
    parts = file_id.split('_')
    product = parts[0]
    sample_str = parts[1].replace('Sample', '')
    sample = int(sample_str)
    direction = parts[2]
    # parts[3] ì´í›„ëŠ” íƒ€ì„ìŠ¤íƒ¬í”„ (ë©”íƒ€ë°ì´í„° íŒŒì‹±ì—ëŠ” ë¶ˆí•„ìš”)

    return {
        'product': product,
        'sample': sample,
        'direction': direction
    }

def is_normal_file(product: str, sample: int) -> bool:
    """
    ì–‘í’ˆ íŒŒì¼ ì—¬ë¶€ íŒë‹¨

    100W: Sample00ë§Œ ì–‘í’ˆ
    200W: Sample03ë§Œ ì–‘í’ˆ
    """
    return sample in NORMAL_SAMPLES.get(product, [])

def load_csv_file(file_path: str) -> pd.DataFrame:
    """
    CSV íŒŒì¼ ë¡œë“œ (7ê°œ ì±„ë„)

    ë©”íƒ€ë°ì´í„°ë¥¼ ê±´ë„ˆë›°ê³  ì‹¤ì œ ë°ì´í„°ë§Œ ì½ê¸°
    ì±„ë„: acc-X, acc-Y, acc-Z, acc-Sum, Gyro-X, Gyro-Y, Gyro-Z
    """
    # ë©”íƒ€ë°ì´í„°ë¥¼ ê±´ë„ˆë›°ê³  ì‹¤ì œ ë°ì´í„°ë§Œ ì½ê¸°
    # "DataSet" ì¤„ ì´í›„ì˜ ë°ì´í„° ì½ê¸°
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # "DataSet" ì¤„ ì°¾ê¸°
    data_start_idx = None
    for i, line in enumerate(lines):
        if line.strip() == 'DataSet':
            data_start_idx = i + 1  # ë‹¤ìŒ ì¤„ì´ í—¤ë”
            break

    if data_start_idx is None:
        raise ValueError(f"'DataSet' ì¤„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

    # í—¤ë” + ë°ì´í„° ì½ê¸°
    df = pd.read_csv(file_path, skiprows=data_start_idx)

    # TimeStamp ì»¬ëŸ¼ ì œê±° (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    if 'TimeStamp' in df.columns:
        df = df.drop(columns=['TimeStamp'])

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì„ í˜• ë³´ê°„)
    df = df.interpolate(method='linear', axis=0).fillna(method='bfill').fillna(method='ffill')

    return df

# ============================================================================
# 4. ìœˆë„ìš° ì„¸ê·¸ë¨¼í…Œì´ì…˜ í•¨ìˆ˜
# ============================================================================

def get_stable_range(total_length: int, margin: float = 0.1) -> Tuple[int, int]:
    """
    ì•ˆì • êµ¬ê°„ ì¶”ì¶œ (ì•/ë’¤ margin% ì œì™¸)

    ì˜ˆ: total_length=10000, margin=0.1 â†’ (1000, 9000)
    """
    start_idx = int(total_length * margin)
    end_idx = int(total_length * (1.0 - margin))
    return start_idx, end_idx

def create_windows(timeseries: pd.DataFrame, fs: float, window_sec: float, hop_sec: float) -> List[pd.DataFrame]:
    """
    ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìœˆë„ìš°ë¡œ ë¶„í• 

    Args:
        timeseries: ì‹œê³„ì—´ ë°ì´í„° (N Ã— 7)
        fs: ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ (Hz)
        window_sec: ìœˆë„ìš° í¬ê¸° (ì´ˆ)
        hop_sec: hop í¬ê¸° (ì´ˆ)

    Returns:
        ìœˆë„ìš° ë¦¬ìŠ¤íŠ¸
    """
    window_size = int(window_sec * fs)
    hop_size = int(hop_sec * fs)

    # ì•ˆì • êµ¬ê°„ ì¶”ì¶œ
    start_idx, end_idx = get_stable_range(len(timeseries), STABLE_MARGIN)
    stable_ts = timeseries.iloc[start_idx:end_idx].reset_index(drop=True)

    windows = []
    for start in range(0, len(stable_ts) - window_size + 1, hop_size):
        end = start + window_size
        window = stable_ts.iloc[start:end].copy()
        windows.append(window)

    return windows

def assign_time_split(window_idx: int, total_windows: int) -> str:
    """
    ìœˆë„ìš°ì˜ ì‹œê°„ ê¸°ë°˜ ë°ì´í„°ì…‹ í• ë‹¹ (ì–‘í’ˆ íŒŒì¼ìš©)

    Args:
        window_idx: ìœˆë„ìš° ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)
        total_windows: ì „ì²´ ìœˆë„ìš° ê°œìˆ˜

    Returns:
        'train', 'val', 'test' ì¤‘ í•˜ë‚˜
    """
    position = window_idx / total_windows

    if position < TIME_SPLIT_RANGES['train'][1]:
        return 'train'
    elif position < TIME_SPLIT_RANGES['val'][1]:
        return 'val'
    else:
        return 'test'

# ============================================================================
# 5. íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
# ============================================================================

def compute_rms(x: np.ndarray) -> float:
    """RMS (Root Mean Square) ê³„ì‚°"""
    return np.sqrt(np.mean(x**2))

def compute_peak(x: np.ndarray) -> float:
    """Peak ê°’ ê³„ì‚° (ì ˆëŒ“ê°’ì˜ ìµœëŒ“ê°’)"""
    return np.max(np.abs(x))

def compute_crest_factor(x: np.ndarray) -> float:
    """Crest Factor ê³„ì‚° (Peak / RMS)"""
    rms_val = compute_rms(x)
    peak_val = compute_peak(x)
    return peak_val / rms_val if rms_val > 0 else 0.0

def butterworth_bandpass(lowcut: float, highcut: float, fs: float, order: int = 4):
    """
    Butterworth ë°´ë“œíŒ¨ìŠ¤ í•„í„° ì„¤ê³„

    Args:
        lowcut: í•˜í•œ ì£¼íŒŒìˆ˜ (Hz)
        highcut: ìƒí•œ ì£¼íŒŒìˆ˜ (Hz)
        fs: ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ (Hz)
        order: í•„í„° ì°¨ìˆ˜

    Returns:
        í•„í„° ê³„ìˆ˜ (b, a)
    """
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = signal.butter(order, [low, high], btype='band')
    return b, a

def compute_band_rms(signal_data: np.ndarray, fs: float, band: Tuple[float, float]) -> float:
    """
    íŠ¹ì • ì£¼íŒŒìˆ˜ ë°´ë“œì˜ RMS ê³„ì‚°

    Args:
        signal_data: ì‹ í˜¸ ë°ì´í„° (1D array)
        fs: ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ (Hz)
        band: ì£¼íŒŒìˆ˜ ë²”ìœ„ (lowcut, highcut)

    Returns:
        ë°´ë“œ RMS ê°’
    """
    lowcut, highcut = band
    b, a = butterworth_bandpass(lowcut, highcut, fs, order=4)

    # í•„í„° ì ìš©
    filtered_signal = signal.filtfilt(b, a, signal_data)

    # RMS ê³„ì‚°
    rms_val = compute_rms(filtered_signal)

    return rms_val

def extract_basic_features(window: pd.DataFrame) -> Dict[str, float]:
    """
    ê¸°ë³¸ í†µê³„ íŠ¹ì§• ì¶”ì¶œ (9ê°œ)

    ì±„ë„: acc-Y, acc-Sum, Gyro-Y
    í†µê³„ëŸ‰: RMS, Peak, Crest Factor
    """
    features = {}

    # acc-Y
    acc_y = window['acc-Y'].values
    features['acc_Y_rms'] = compute_rms(acc_y)
    features['acc_Y_peak'] = compute_peak(acc_y)
    features['acc_Y_crest'] = compute_crest_factor(acc_y)

    # acc-Sum
    acc_sum = window['acc-Sum'].values
    features['acc_Sum_rms'] = compute_rms(acc_sum)
    features['acc_Sum_peak'] = compute_peak(acc_sum)
    features['acc_Sum_crest'] = compute_crest_factor(acc_sum)

    # Gyro-Y
    gyro_y = window['Gyro-Y'].values
    features['Gyro_Y_rms'] = compute_rms(gyro_y)
    features['Gyro_Y_peak'] = compute_peak(gyro_y)
    features['Gyro_Y_crest'] = compute_crest_factor(gyro_y)

    return features

def extract_band_rms_features(window: pd.DataFrame, fs: float) -> Dict[str, float]:
    """
    ë°´ë“œ RMS íŠ¹ì§• ì¶”ì¶œ (9ê°œ)

    ì±„ë„: acc-Y, acc-Sum, Gyro-Y
    ë°´ë“œ: Low (1-10 Hz), Mid (10-50 Hz), High (50-150 Hz)
    """
    features = {}

    # ê° ì±„ë„ì— ëŒ€í•´
    for channel_name, column_name in [('acc_Y', 'acc-Y'), ('acc_Sum', 'acc-Sum'), ('Gyro_Y', 'Gyro-Y')]:
        signal_data = window[column_name].values

        # ê° ì£¼íŒŒìˆ˜ ë°´ë“œì— ëŒ€í•´
        for band_name, band_range in FREQUENCY_BANDS.items():
            feature_name = f"{channel_name}_rms_{band_name}"
            features[feature_name] = compute_band_rms(signal_data, fs, band_range)

    return features

def extract_window_features(window: pd.DataFrame, fs: float) -> Dict[str, float]:
    """
    ìœˆë„ìš°ì—ì„œ ëª¨ë“  íŠ¹ì§• ì¶”ì¶œ (18ê°œ)

    Returns:
        9ê°œ ê¸°ë³¸ í†µê³„ëŸ‰ + 9ê°œ ë°´ë“œ RMS = 18ê°œ íŠ¹ì§•
    """
    features = {}

    # ê¸°ë³¸ í†µê³„ íŠ¹ì§• (9ê°œ)
    basic_feat = extract_basic_features(window)
    features.update(basic_feat)

    # ë°´ë“œ RMS íŠ¹ì§• (9ê°œ)
    band_feat = extract_band_rms_features(window, fs)
    features.update(band_feat)

    return features

# ============================================================================
# 6. ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================================================

def process_file(
    row: pd.Series,
    fs: float,
    label_raw: str,
    label_binary: int,
    label_weight: float
) -> List[Dict]:
    """
    ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬: CSV ë¡œë“œ â†’ ìœˆë„ìš° ìƒì„± â†’ íŠ¹ì§• ì¶”ì¶œ

    Args:
        row: ecmDataì˜ í•œ í–‰ (file_path, file_id)
        fs: ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜
        label_raw: ì›ë³¸ ë¼ë²¨ (ì •ìƒ/ì†ŒìŒ/ì§„ë™/í‘œê¸°ì—†ìŒ)
        label_binary: ì´ì§„ ë¼ë²¨ (1=ì–‘í’ˆ, 0=ë¶ˆëŸ‰)
        label_weight: ë¼ë²¨ ê°€ì¤‘ì¹˜ (í•™ìŠµìš©)

    Returns:
        ìœˆë„ìš°ë³„ íŠ¹ì§• ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    file_path_str = row['file_path']
    file_id = row['file_id']

    # íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬ (ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ)
    file_path = Path(file_path_str)
    if not file_path.is_absolute():
        # ìƒëŒ€ ê²½ë¡œëŠ” PROJECT_ROOT ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
        file_path = PROJECT_ROOT / file_path

    # ë©”íƒ€ë°ì´í„° íŒŒì‹±
    meta = parse_file_id(file_id)
    product = meta['product']
    sample = meta['sample']
    direction = meta['direction']

    # CSV íŒŒì¼ ë¡œë“œ
    timeseries = load_csv_file(str(file_path))

    # ìœˆë„ìš° ìƒì„±
    windows = create_windows(timeseries, fs, WINDOW_SEC, HOP_SEC)

    # ê° ìœˆë„ìš° ì²˜ë¦¬
    window_features_list = []

    for window_idx, window in enumerate(windows):
        # ë°ì´í„°ì…‹ í• ë‹¹
        if is_normal_file(product, sample):
            # ì–‘í’ˆ: ì‹œê°„ ê¸°ë°˜ ë¶„í• 
            dataset_type = assign_time_split(window_idx, len(windows))
        else:
            # ë¶ˆëŸ‰: trainìœ¼ë¡œ ê¸°ë³¸ ì„¤ì • (ë‚˜ì¤‘ì— íŒŒì¼ ë‹¨ìœ„ë¡œ ì¬ë¶„í• )
            dataset_type = 'train'

        # íŠ¹ì§• ì¶”ì¶œ
        features = extract_window_features(window, fs)

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        window_data = {
            'window_id': f"{file_id}_win{window_idx:03d}",
            'file_id': file_id,
            'dataset_type': dataset_type,
            'label_raw': label_raw,
            'label_binary': label_binary,
            'label_weight': label_weight,
            'product': product,
            'sample': sample,
            'direction': direction
        }
        window_data.update(features)

        window_features_list.append(window_data)

    return window_features_list

def assign_abnormal_splits(df: pd.DataFrame) -> pd.DataFrame:
    """
    ë¶ˆëŸ‰ íŒŒì¼ì˜ ìœˆë„ìš°ë¥¼ train/val/testë¡œ ë¶„í• 

    Args:
        df: ì „ì²´ ìœˆë„ìš° ë°ì´í„°í”„ë ˆì„

    Returns:
        dataset_typeì´ í• ë‹¹ëœ ë°ì´í„°í”„ë ˆì„
    """
    # ì–‘í’ˆì€ ì´ë¯¸ ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ í• ë‹¹ë˜ì–´ ìˆìŒ
    normal_df = df[df['label_binary'] == 1].copy()

    # ë¶ˆëŸ‰ ìœˆë„ìš°ë§Œ ì¶”ì¶œ
    abnormal_df = df[df['label_binary'] == 0].copy()

    if len(abnormal_df) == 0:
        return df

    # íŒŒì¼ IDë³„ë¡œ ê·¸ë£¹í™”
    file_ids = abnormal_df['file_id'].unique()
    np.random.seed(RANDOM_SEED)
    np.random.shuffle(file_ids)

    # íŒŒì¼ ë‹¨ìœ„ë¡œ train/val/test ë¶„í• 
    n_files = len(file_ids)
    n_train = int(n_files * ABNORMAL_SPLIT_RATIOS['train'])
    n_val = int(n_files * ABNORMAL_SPLIT_RATIOS['val'])

    train_files = file_ids[:n_train]
    val_files = file_ids[n_train:n_train+n_val]
    test_files = file_ids[n_train+n_val:]

    # íŒŒì¼ IDì— ë”°ë¼ dataset_type í• ë‹¹
    abnormal_df.loc[abnormal_df['file_id'].isin(train_files), 'dataset_type'] = 'train'
    abnormal_df.loc[abnormal_df['file_id'].isin(val_files), 'dataset_type'] = 'val'
    abnormal_df.loc[abnormal_df['file_id'].isin(test_files), 'dataset_type'] = 'test'

    # ì–‘í’ˆê³¼ ë¶ˆëŸ‰ í•©ì¹˜ê¸°
    result_df = pd.concat([normal_df, abnormal_df], ignore_index=True)

    return result_df

# ============================================================================
# 7. ì‹¤í–‰ ì½”ë“œ
# ============================================================================

print("\n[1ë‹¨ê³„] ì…ë ¥ CSV ë¡œë“œ")
print("-" * 60)

# ì…ë ¥ CSV ë¡œë“œ (ecminer_input_full.csv)
if not INPUT_CSV_PATH.exists():
    raise FileNotFoundError(
        f"ì…ë ¥ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_CSV_PATH}\n"
        f"ë¨¼ì € create_ecminer_input.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ì…ë ¥ íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”."
    )

ecmData = pd.read_csv(INPUT_CSV_PATH)
print(f"âœ“ ì…ë ¥ CSV ë¡œë“œ ì™„ë£Œ: {len(ecmData)}ê°œ íŒŒì¼")

# í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
required_columns = ['file_path', 'file_id']
for col in required_columns:
    if col not in ecmData.columns:
        raise ValueError(f"ì…ë ¥ ë°ì´í„°ì— '{col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

# ============================================================================
# ë ˆì´ë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
# ============================================================================

print("\n[2ë‹¨ê³„] ë ˆì´ë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
print("-" * 60)

# ë ˆì´ë¸” íŒŒì¼ ë¡œë“œ
if not LABEL_CSV_PATH.exists():
    raise FileNotFoundError(f"ë ˆì´ë¸” íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LABEL_CSV_PATH}")

labels_df = load_label_file(LABEL_CSV_PATH)
print(f"âœ“ ë ˆì´ë¸” íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(labels_df)}ê°œ ë ˆì´ë¸”")

# ì„¤ì • íŒŒì¼ ë¡œë“œ
if not CONFIG_YAML_PATH.exists():
    raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CONFIG_YAML_PATH}")

config = load_config(CONFIG_YAML_PATH)
strategy_name = config['label_strategy']
strategy = config['strategies'][strategy_name]
print(f"âœ“ ë ˆì´ë¸” ì „ëµ: {strategy_name} - {strategy.get('description', '')}")

# ============================================================================
# ì „ì²´ íŒŒì¼ ì²˜ë¦¬
# ============================================================================

print("\n[3ë‹¨ê³„] íŒŒì¼ ì²˜ë¦¬")
print("-" * 60)

all_windows = []
processed_count = 0
error_count = 0

for idx, row in ecmData.iterrows():
    try:
        # file_idì—ì„œ product, sample ì¶”ì¶œ
        meta = parse_file_id(row['file_id'])
        product = meta['product']
        sample = meta['sample']

        # ë ˆì´ë¸” ë§¤ì¹­
        label_row = labels_df[
            (labels_df['product'] == product) &
            (labels_df['sample'] == sample)
        ]

        if len(label_row) == 0:
            print(f"ê²½ê³ : ë ˆì´ë¸” ì—†ìŒ, ê±´ë„ˆëœ€: {product} Sample{sample:02d}")
            error_count += 1
            continue

        label_raw = label_row['label'].iloc[0]

        # ë ˆì´ë¸” ì „ëµ ì ìš©
        label_binary, label_weight = apply_label_strategy(
            label_raw, product, sample, strategy
        )

        # íŒŒì¼ ì²˜ë¦¬
        windows = process_file(
            row, SAMPLING_FREQ,
            label_raw=label_raw,
            label_binary=label_binary,
            label_weight=label_weight
        )
        all_windows.extend(windows)
        processed_count += 1

        if processed_count % 10 == 0:
            print(f"  ì§„í–‰ì¤‘: {processed_count}/{len(ecmData)} íŒŒì¼ ì²˜ë¦¬ë¨...")

    except Exception as e:
        print(f"ê²½ê³ : íŒŒì¼ {row['file_id']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        error_count += 1
        continue

print(f"âœ“ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")

# ë°ì´í„°í”„ë ˆì„ ìƒì„±
output_df = pd.DataFrame(all_windows)

# ë¶ˆëŸ‰ íŒŒì¼ ë°ì´í„°ì…‹ ë¶„í• 
output_df = assign_abnormal_splits(output_df)

# ê²°ê³¼ ì •ë ¬ (train â†’ val â†’ test ìˆœì„œ)
dataset_order = {'train': 0, 'val': 1, 'test': 2}
output_df['_sort_key'] = output_df['dataset_type'].map(dataset_order)
output_df = output_df.sort_values('_sort_key').drop(columns=['_sort_key']).reset_index(drop=True)

# ============================================================================
# ê²°ê³¼ ì¶œë ¥
# ============================================================================

print("\n[4ë‹¨ê³„] ì²˜ë¦¬ ì™„ë£Œ")
print("-" * 60)
print(f"  - ì´ ìœˆë„ìš° ìˆ˜: {len(output_df)}")
print(f"  - Train: {len(output_df[output_df['dataset_type']=='train'])}")
print(f"  - Val: {len(output_df[output_df['dataset_type']=='val'])}")
print(f"  - Test: {len(output_df[output_df['dataset_type']=='test'])}")
print(f"  - ì •ìƒ: {len(output_df[output_df['label_binary']==1])}")
print(f"  - ë¶ˆëŸ‰: {len(output_df[output_df['label_binary']==0])}")
print(f"  - íŠ¹ì§• ê°œìˆ˜: {len(BASIC_FEATURES + BAND_RMS_FEATURES)}ê°œ")

# NaN ê²€ì‚¬
if output_df.isnull().any().any():
    print("ê²½ê³ : ê²°ê³¼ì— NaN ê°’ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    nan_cols = output_df.columns[output_df.isnull().any()].tolist()
    print(f"  NaN í¬í•¨ ì»¬ëŸ¼: {nan_cols}")

# ì¤‘ë³µ window_id ê²€ì‚¬
if output_df['window_id'].duplicated().any():
    n_duplicates = output_df['window_id'].duplicated().sum()
    print(f"ê²½ê³ : {n_duplicates}ê°œì˜ ì¤‘ë³µ window_idê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("  âœ“ ì¤‘ë³µ window_id ì—†ìŒ")

# ECMinerê°€ ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬í•  ë°ì´í„°
ecmData = output_df

# íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒì‚¬í•­)
if OUTPUT_CSV_PATH is not None:
    output_df.to_csv(OUTPUT_CSV_PATH, index=False)
    print(f"\nâœ“ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {OUTPUT_CSV_PATH}")

print("\n" + "=" * 60)
print("ECMiner ì¶œë ¥ ì¤€ë¹„ ì™„ë£Œ!")
print("=" * 60)
