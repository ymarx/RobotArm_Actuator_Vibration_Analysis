# Phase 1 ì™„ë£Œ ë¶„ì„ ë³´ê³ ì„œ

**ì‘ì„±ì¼**: 2025-11-17
**ë¶„ì„ ë²”ìœ„**: ë°ì´í„° ì „ì²˜ë¦¬ ê²€ì¦ ë° íƒìƒ‰ì  ë¶„ì„
**ë‹¤ìŒ ë‹¨ê³„**: Phase 2 (XGBoost Baseline ëª¨ë¸ë§) ì¤€ë¹„

---

## ğŸ“Š Executive Summary

### âœ… ì„±ê³µì ì¸ ê°œì„ ì‚¬í•­
- **í’ˆì§ˆ ê¸°ì¤€ ì¡°ì •**: ì œí’ˆë³„ ì¸¡ì • ì‹œê°„ ì°¨ì´ ë°˜ì˜ â†’ ì‚¬ìš© ê°€ëŠ¥ íŒŒì¼ 51.7% â†’ **95.0%** í–¥ìƒ
- **ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦**: 5ê°€ì§€ ê²€ì¦ í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼ â†’ ë°ì´í„° ë¶„í•  ë¬´ê²°ì„± í™•ì¸
- **Feature ì¶”ì¶œ**: 49ê°œ ì‹œê°„ ì˜ì—­ feature ì„±ê³µì  ìƒì„±
- **í´ë˜ìŠ¤ ë¶„ë¦¬ ê°€ëŠ¥ì„±**: 28/49 featuresê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ (p<0.05)

### âš ï¸ ì£¼ìš” ì´ìŠˆ
1. **ë¶ˆê· í˜•í•œ ë°ì´í„°ì…‹**: ì •ìƒ 13ê°œ vs ë¶ˆëŸ‰ 47ê°œ íŒŒì¼ (1:3.6 ë¹„ìœ¨)
2. **ê²°ì¸¡ì¹˜**: 5ê°œ ìœˆë„ìš°ì—ì„œ 49ê°œ feature ëª¨ë‘ NaN (200W_S02_CW_R4)
3. **ë‚®ì€ Effect Size**: Large effect (|d|>0.8) feature 0ê°œ â†’ í´ë˜ìŠ¤ ë¶„ë¦¬ ì–´ë ¤ì›€ ì˜ˆìƒ
4. **ê³ ìƒê´€ Features**: 31ê°œ feature ìŒì´ |r|>0.9 â†’ ì¤‘ë³µì„± ë†’ìŒ

---

## 1ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ ê²€ì¦ ê²°ê³¼

### 1.1 í’ˆì§ˆ ê²€ì‚¬ í†µê³¼ìœ¨

| í•­ëª© | ê²°ê³¼ | ë¹„ê³  |
|-----|------|------|
| **ì´ íŒŒì¼** | 60ê°œ | 100W: 30ê°œ, 200W: 30ê°œ |
| **ì‚¬ìš© ê°€ëŠ¥** | 57ê°œ (95.0%) | âœ… ëª©í‘œ ë‹¬ì„± |
| **í’ˆì§ˆ ì‹¤íŒ¨** | 3ê°œ (5.0%) | 200W_S02 3ê°œ íŒŒì¼ |

**ì œí’ˆë³„ í†µê³¼ìœ¨**:
- **100W**: 30/30 (100%) âœ…
- **200W**: 27/30 (90.0%) âš ï¸

**í’ˆì§ˆ ì‹¤íŒ¨ íŒŒì¼**:
```
200W_S02_CCW_R1: 43,689 samples (expected: 32k-36k) - 21% ì´ˆê³¼
200W_S02_CW_R1:  59,462 samples (expected: 32k-36k) - 65% ì´ˆê³¼
200W_S02_CW_R4:  53,994 samples (expected: 32k-36k) - 50% ì´ˆê³¼
```

### 1.2 ë¼ë²¨ ë¶„í¬

#### ì›ë³¸ ë¼ë²¨ (Raw)
```
í‘œê¸°ì—†ìŒ: 38ê°œ (63.3%)
ì†ŒìŒ:      9ê°œ (15.0%)
ì§„ë™:      9ê°œ (15.0%)
ì •ìƒ:      4ê°œ (6.7%)  â† ì–‘í’ˆ íŒŒì¼ ë§¤ìš° ë¶€ì¡±
```

#### ì´ì§„ ë¶„ë¥˜ ë¼ë²¨
```
ë¶ˆëŸ‰ (0): 47ê°œ íŒŒì¼ (78.3%)
ì •ìƒ (1): 13ê°œ íŒŒì¼ (21.7%)  â† 200W_S03ì˜ ì†ŒìŒ 9ê°œ â†’ ì–‘í’ˆ ì¬ë¶„ë¥˜

ì •ìƒ:ë¶ˆëŸ‰ ë¹„ìœ¨ = 1:3.6
```

### 1.3 ë°ì´í„° ë¶„í•  ê²€ì¦ âœ…

**5ê°€ì§€ ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦ í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼**:

1. âœ… **íŒŒì¼ ë ˆë²¨ ëˆ„ìˆ˜ ì—†ìŒ**: ë™ì¼ íŒŒì¼ì˜ ìœˆë„ìš°ê°€ ì—¬ëŸ¬ splitì— ë“±ì¥í•˜ì§€ ì•ŠìŒ (time_split ì œì™¸)
2. âœ… **ì‹œê°„ ë¶„í•  ê²½ê³„ ì¤€ìˆ˜**: time_split ìœˆë„ìš°ê°€ ì§€ì •ëœ ì‹œê°„ êµ¬ê°„ ë‚´ì— ìœ„ì¹˜
3. âœ… **ë°©í–¥ ì¼ê´€ì„±**: ë™ì¼ íŒŒì¼ ë‚´ CW/CCW ë°©í–¥ ì¼ê´€ë¨
4. âœ… **Val/Test ë…ë¦½ì„±**: Balancing í›„ì—ë„ val/test ì„¸íŠ¸ ë³€ê²½ ì—†ìŒ
5. âœ… **ì‹œê°„ ì¤‘ë³µ ì—†ìŒ**: Split ê°„ ìœˆë„ìš° ì‹œê°„ ì¤‘ë³µ ì—†ìŒ

### 1.4 ìœˆë„ìš° ë¶„í•  ê²°ê³¼

| Split | ì •ìƒ | ë¶ˆëŸ‰ | ì´ ìœˆë„ìš° | ë¹„ìœ¨ |
|-------|------|------|----------|------|
| **Train** | 160 (33.3%) | 321 (66.7%) | 481 | 1:2.0 (balancing ì ìš©) |
| **Val** | 4 (4.3%) | 88 (95.7%) | 92 | 1:22.0 âš ï¸ |
| **Test** | 14 (12.6%) | 97 (87.4%) | 111 | 1:6.9 âš ï¸ |
| **Total** | 178 | 506 | 684 | 1:2.8 |

**âš ï¸ Val/Test ë¶ˆê· í˜• ì´ìŠˆ**:
- Validation setì— ì •ìƒ ìœˆë„ìš° 4ê°œë§Œ ì¡´ì¬ â†’ ì‹ ë¢°ì„± ìˆëŠ” ê²€ì¦ ì–´ë ¤ì›€
- Test setë„ ì •ìƒ 14ê°œë¡œ ë¶€ì¡± â†’ ì¼ë°˜í™” í‰ê°€ ì œí•œì 

---

## 2ï¸âƒ£ Feature íƒìƒ‰ì  ë¶„ì„ ê²°ê³¼

### 2.1 Feature êµ¬ì„±

**ì´ 62ê°œ ì»¬ëŸ¼**:
- **ì„¼ì„œ Feature**: 49ê°œ (acc-X/Y/Z/Sum + Gyro-X/Y/Z)
- **ë©”íƒ€ Feature**: 11ê°œ (window_id, file_id, split_set, product, sample, direction, labels ë“±)
- **ìœˆë„ìš° ì •ë³´**: 2ê°œ (window_duration, window_num_samples)

**ì±„ë„ë³„ Feature**:
- Acceleration (X/Y/Z/Sum): 7ê°œ í†µê³„ëŸ‰ Ã— 4ì±„ë„ = 28ê°œ
- Gyroscope (X/Y/Z): 7ê°œ í†µê³„ëŸ‰ Ã— 3ì±„ë„ = 21ê°œ

**í†µê³„ëŸ‰**: mean, std, rms, peak, crest_factor, kurtosis, skewness

### 2.2 í´ë˜ìŠ¤ ë¶„ë¦¬ ê°€ëŠ¥ì„± ë¶„ì„ (Train Set)

#### í†µê³„ì  ìœ ì˜ì„±
- **Significant features** (p<0.05): **28/49** (57.1%)
- **Large effect size** (|d|>0.8): **0/49** (0%) âš ï¸
- **Medium effect size** (0.5â‰¤|d|<0.8): **2/49** (4.1%)

#### Top 10 êµ¬ë³„ë ¥ ë†’ì€ Features (Cohen's d ê¸°ì¤€)

| Rank | Feature | Cohen's d | p-value | í•´ì„ |
|------|---------|-----------|---------|------|
| 1 | **Gyro_Y_std** | -0.599 | 7.08e-12 | ë¶ˆëŸ‰í’ˆì—ì„œ Yì¶• íšŒì „ ë³€ë™ í¼ |
| 2 | **acc_Y_skewness** | -0.612 | 1.36e-09 | ë¶ˆëŸ‰í’ˆì—ì„œ Yì¶• ê°€ì†ë„ ë¹„ëŒ€ì¹­ |
| 3 | **acc_Y_rms** | -0.477 | 4.78e-09 | ë¶ˆëŸ‰í’ˆì—ì„œ Yì¶• ì§„ë™ í¬ê¸° í¼ |
| 4 | **acc_Y_std** | -0.472 | 6.61e-09 | ë¶ˆëŸ‰í’ˆì—ì„œ Yì¶• ì§„ë™ ë³€ë™ í¼ |
| 5 | **acc_Sum_mean** | -0.474 | 7.61e-09 | ë¶ˆëŸ‰í’ˆì—ì„œ ì „ì²´ ê°€ì†ë„ í‰ê·  ë†’ìŒ |
| 6 | **Gyro_X_std** | -0.535 | 8.78e-09 | ë¶ˆëŸ‰í’ˆì—ì„œ Xì¶• íšŒì „ ë³€ë™ í¼ |
| 7 | **acc_Sum_rms** | -0.469 | 1.01e-08 | ë¶ˆëŸ‰í’ˆì—ì„œ ì „ì²´ ì§„ë™ í¬ê¸° í¼ |
| 8 | **acc_X_peak** | -0.472 | 1.10e-08 | ë¶ˆëŸ‰í’ˆì—ì„œ Xì¶• ìµœëŒ€ ì§„í­ í¼ |
| 9 | **acc_Sum_peak** | -0.468 | 1.22e-08 | ë¶ˆëŸ‰í’ˆì—ì„œ ì „ì²´ ìµœëŒ€ ì§„í­ í¼ |
| 10 | **acc_X_rms** | -0.470 | 1.37e-08 | ë¶ˆëŸ‰í’ˆì—ì„œ Xì¶• ì§„ë™ í¬ê¸° í¼ |

**íŒ¨í„´ ë¶„ì„**:
- ğŸ”´ **ëª¨ë“  Cohen's dê°€ ìŒìˆ˜** â†’ ë¶ˆëŸ‰í’ˆì´ ì •ìƒí’ˆë³´ë‹¤ ì§„ë™/íšŒì „ í¬ê¸°ê°€ í¼
- ğŸ“Š **Yì¶• ê´€ë ¨ featureê°€ ìƒìœ„ê¶Œ** â†’ Yì¶• ë°©í–¥ ì§„ë™ì´ ë¶ˆëŸ‰ ê°ì§€ì— ì¤‘ìš”
- ğŸ”„ **Gyroscope featureë„ ìƒìœ„** â†’ íšŒì „ ìš´ë™ íŒ¨í„´ì´ ë¶ˆëŸ‰ íŠ¹ì„± ë°˜ì˜
- âš ï¸ **Effect sizeê°€ ëª¨ë‘ small-medium** â†’ ë‹¨ì¼ featureë¡œ ì™„ë²½í•œ ë¶„ë¥˜ ì–´ë ¤ì›€

### 2.3 Feature ìƒê´€ê´€ê³„ ë¶„ì„

#### ê³ ìƒê´€ Feature ìŒ (|r| > 0.9)
- **ì´ 31ê°œ ìŒ ë°œê²¬**
- ì£¼ë¡œ **ë™ì¼ ì±„ë„ ë‚´ std â†” rms â†” peak** ê°„ ê°•í•œ ìƒê´€

**ì˜ˆì‹œ**:
```
acc_X_std â†” acc_X_rms:  r = 0.9999  â† ê±°ì˜ ì¤‘ë³µ
acc_Y_std â†” acc_Y_rms:  r = 0.9997
acc_X_std â†” acc_X_peak: r = 0.9698
acc_X_std â†” acc_Z_std:  r = 0.9537  â† ì¶• ê°„ ìƒê´€
```

**í•¨ì˜**:
- ğŸ“‰ **Feature ì¤‘ë³µì„± ë†’ìŒ** â†’ ì°¨ì› ì¶•ì†Œ í•„ìš” (PCA, feature selection)
- ğŸ¯ **std, rms, peak ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ ê°€ëŠ¥** â†’ ëª¨ë¸ ë³µì¡ë„ ê°ì†Œ
- âš¡ **ë‹¤ì¤‘ê³µì„ ì„± ì´ìŠˆ** â†’ Linear modelë³´ë‹¤ Tree-based model ìœ ë¦¬

### 2.4 ì´ìƒì¹˜ (Outlier) ë¶„ì„

**ì´ìƒì¹˜ê°€ ìˆëŠ” Features**: 40/49 (81.6%)

**Top 5 Outlier Features**:
| Feature | Outlier ë¹„ìœ¨ | í•´ì„ |
|---------|-------------|------|
| Gyro_Z_kurtosis | 10.1% | Zì¶• íšŒì „ ì²¨ë„ ê·¹ê°’ ë§ìŒ |
| acc_Sum_std | 7.8% | ì „ì²´ ì§„ë™ ë³€ë™ ê·¹ê°’ |
| acc_Y_std | 7.8% | Yì¶• ì§„ë™ ë³€ë™ ê·¹ê°’ |
| acc_Y_rms | 7.8% | Yì¶• ì§„ë™ í¬ê¸° ê·¹ê°’ |
| acc_Y_peak | 7.1% | Yì¶• ìµœëŒ€ ì§„í­ ê·¹ê°’ |

**ì´ìƒì¹˜ ì²˜ë¦¬ ë°©ì•ˆ**:
- âœ… **XGBoostëŠ” outlierì— robust** â†’ ì œê±°í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ í•™ìŠµ
- ğŸ“Š **ì´ìƒì¹˜ê°€ ë¶ˆëŸ‰í’ˆ íŠ¹ì§•ì¼ ê°€ëŠ¥ì„±** â†’ ì œê±°í•˜ë©´ ì •ë³´ ì†ì‹¤ ìš°ë ¤
- ğŸ” **ëª¨ë¸ í•™ìŠµ í›„ feature importanceë¡œ ê²€ì¦** í•„ìš”

### 2.5 ê²°ì¸¡ì¹˜ (Missing Values)

**ê²°ì¸¡ ìœˆë„ìš°**: 5ê°œ (ì „ì²´ì˜ 0.7%)
- **íŒŒì¼**: 200W_S02_CW_R4
- **ì˜í–¥**: 49ê°œ feature ëª¨ë‘ NaN

**ì²˜ë¦¬ ë°©ì•ˆ**:
1. **ì œê±° (ê¶Œì¥)**: 0.7%ë¡œ ì˜í–¥ ë¯¸ë¯¸, ë‹¨ìˆœ ì œê±°
2. **Imputation**: Mean/Median ëŒ€ì²´ (ë¹„ê¶Œì¥ - ì •ë³´ ì™œê³¡)

---

## 3ï¸âƒ£ ì œí’ˆ/ë°©í–¥ë³„ íŠ¹ì„± ë¹„êµ

### 3.1 ì œí’ˆë³„ ë¹„êµ (100W vs 200W)

**íŒŒì¼ ë¶„í¬**:
- 100W: 30ê°œ íŒŒì¼ (ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥)
- 200W: 30ê°œ íŒŒì¼ (27ê°œ ì‚¬ìš© ê°€ëŠ¥, 3ê°œ í’ˆì§ˆ ì‹¤íŒ¨)

**ì¸¡ì • ì‹œê°„ ì°¨ì´**:
- 100W: ~27k-29k samples (~53-57ì´ˆ)
- 200W: ~32k-36k samples (~62-68ì´ˆ) â† **ë” ê¸´ ì¸¡ì •**

**Feature ì°¨ì´ ë¶„ì„** í•„ìš”:
- ì œí’ˆ ì¶œë ¥(100W vs 200W)ì— ë”°ë¥¸ ì§„ë™ íŒ¨í„´ ì°¨ì´
- ëª¨ë¸ í•™ìŠµ ì‹œ `product` feature í¬í•¨ ì—¬ë¶€ ê²€í† 

### 3.2 ë°©í–¥ë³„ ë¹„êµ (CW vs CCW)

**íŒŒì¼ ë¶„í¬**:
- CW (ì‹œê³„ë°©í–¥): 30ê°œ
- CCW (ë°˜ì‹œê³„ë°©í–¥): 30ê°œ

**Feature ì°¨ì´ ë¶„ì„** í•„ìš”:
- íšŒì „ ë°©í–¥ì— ë”°ë¥¸ ì§„ë™/íšŒì „ íŒ¨í„´ ì°¨ì´
- Gyroscope sign ë°˜ì „ ì—¬ë¶€ í™•ì¸

---

## 4ï¸âƒ£ Phase 2 ì¤€ë¹„ì‚¬í•­ ë° ê¶Œì¥ ì‚¬í•­

### 4.1 ë°ì´í„° ì „ì²˜ë¦¬ ìµœì¢… ì¡°ì¹˜

#### âœ… ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”
1. **ê²°ì¸¡ì¹˜ ì²˜ë¦¬**:
   ```python
   # 5ê°œ ìœˆë„ìš° ì œê±° (0.7% ì˜í–¥ ë¯¸ë¯¸)
   features_clean = features.dropna()
   ```

2. **í’ˆì§ˆ ì‹¤íŒ¨ íŒŒì¼ ì²˜ë¦¬**:
   - í˜„ì¬: ìë™ ì œì™¸ë¨ (file_masterì—ì„œ quality_pass=False)
   - í™•ì¸: 200W_S02ì˜ 3ê°œ íŒŒì¼ì´ ëª¨ë¸ë§ì—ì„œ ì œì™¸ë˜ì—ˆëŠ”ì§€ ê²€ì¦

#### ğŸ” ê²€í†  ì‚¬í•­
3. **Feature ì¤‘ë³µì„± í•´ê²°**:
   - **Option A**: ê³ ìƒê´€ ìŒ ì¤‘ í•˜ë‚˜ ì œê±° (ìˆ˜ë™ ì„ íƒ)
   - **Option B**: XGBoost feature_importance ê¸°ë°˜ ì„ íƒ
   - **Option C**: PCAë¡œ ì°¨ì› ì¶•ì†Œ
   - **ê¶Œì¥**: Option B (ëª¨ë¸ ê¸°ë°˜ ì„ íƒ)

4. **Val/Test ë¶ˆê· í˜• í•´ê²°**:
   - **í˜„ìƒ**: Valì— ì •ìƒ 4ê°œ, Testì— ì •ìƒ 14ê°œë§Œ ì¡´ì¬
   - **ì›ì¸**: ì–‘í’ˆ íŒŒì¼ ë¶€ì¡± (100W_S00, 200W_S03ë§Œ ì¡´ì¬)
   - **í•´ê²°ì±…**:
     - **Option A**: Cross-validation (StratifiedKFold) ì‚¬ìš© â† **ê¶Œì¥**
     - **Option B**: ì‹œê°„ ë¶„í•  ë¹„ìœ¨ ì¬ì¡°ì • (train 50%, val 20%, test 30%)
     - **Option C**: ë°ì´í„° ì¶”ê°€ ìˆ˜ì§‘ (ì¥ê¸° ê³„íš)

### 4.2 Phase 2 ëª¨ë¸ë§ ì „ëµ

#### Baseline Model: XGBoost

**ì¥ì **:
- âœ… Tree-based â†’ ë‹¤ì¤‘ê³µì„ ì„±ì— robust
- âœ… Outlierì— ê°•ê±´
- âœ… Feature importance ì œê³µ
- âœ… Class imbalance ì²˜ë¦¬ ê°€ëŠ¥ (`scale_pos_weight`)

**í•™ìŠµ ì„¤ì •**:
```python
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'scale_pos_weight': 321/160,  # ë¶ˆëŸ‰/ì •ìƒ ë¹„ìœ¨
    'max_depth': 6,
    'learning_rate': 0.01,
    'n_estimators': 1000,
    'early_stopping_rounds': 50
}
```

**í‰ê°€ ì§€í‘œ**:
- **Primary**: AUC-ROC (ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©)
- **Secondary**: Precision, Recall, F1-score
- **Threshold ì¡°ì •**: ë¶ˆëŸ‰í’ˆ íƒì§€ìœ¨(Recall) ìš°ì„  vs ì˜¤íƒë¥ (FPR) ìµœì†Œí™”

#### Feature Engineering ì¶”ê°€ ê²€í† 

**í˜„ì¬ ë¶€ì¡±í•œ ë¶€ë¶„**:
1. **ì£¼íŒŒìˆ˜ ì˜ì—­ features** â† **ê°€ì¥ ì¤‘ìš”**
   - FFT peak frequencies
   - Power spectral density
   - Spectral entropy
   - Dominant frequency

2. **ê³ ê¸‰ ì‹œê°„ ì˜ì—­ features**:
   - Zero-crossing rate
   - Autocorrelation
   - Wavelet coefficients

3. **Cross-channel features**:
   - X-Y-Z ê°„ ìƒê´€ê³„ìˆ˜
   - ì¶• ê°„ phase lag

**ìš°ì„ ìˆœìœ„**:
1ï¸âƒ£ XGBoost Baseline ë¨¼ì € í•™ìŠµ (í˜„ì¬ 49ê°œ feature)
2ï¸âƒ£ Feature importance ë¶„ì„
3ï¸âƒ£ ì£¼íŒŒìˆ˜ ì˜ì—­ feature ì¶”ê°€ í›„ ì„±ëŠ¥ ë¹„êµ

### 4.3 ê²€ì¦ ì „ëµ

#### Cross-Validation
```python
from sklearn.model_selection import StratifiedKFold

# 5-Fold CV (ì •ìƒ ìƒ˜í”Œ ë¶€ì¡±ìœ¼ë¡œ ì‘ì€ k ì‚¬ìš©)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Train setë§Œ CV ìˆ˜í–‰
for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
```

#### Holdout Test Set
- í˜„ì¬ test set (111 ìœˆë„ìš°) ìµœì¢… í‰ê°€ìš©ìœ¼ë¡œ ë³´ì¡´
- **ì£¼ì˜**: ì •ìƒ 14ê°œë§Œ ì¡´ì¬ â†’ ì •ìƒ ë¶„ë¥˜ ì„±ëŠ¥ ë¶ˆì•ˆì • ê°€ëŠ¥ì„±

### 4.4 ë‹¤ìŒ ë‹¨ê³„ Action Items

#### Week 1: XGBoost Baseline
- [ ] ê²°ì¸¡ì¹˜ 5ê°œ ìœˆë„ìš° ì œê±°
- [ ] XGBoost baseline ëª¨ë¸ í•™ìŠµ (5-Fold CV)
- [ ] Feature importance ë¶„ì„
- [ ] ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (AUC, Precision, Recall, F1)
- [ ] Confusion matrix ë° ROC curve ì‹œê°í™”

#### Week 2: Feature Engineering
- [ ] ì£¼íŒŒìˆ˜ ì˜ì—­ feature ì¶”ì¶œ (FFT-based)
- [ ] Feature selection (ì¤‘ë³µ feature ì œê±°)
- [ ] ê°œì„ ëœ ëª¨ë¸ ì¬í•™ìŠµ ë° ì„±ëŠ¥ ë¹„êµ

#### Week 3: Model Optimization
- [ ] Hyperparameter tuning (Optuna)
- [ ] Threshold optimization (Recall ìš°ì„  vs Precision ìš°ì„ )
- [ ] ìµœì¢… ëª¨ë¸ ì„ ì • ë° test set í‰ê°€

#### Week 4: Advanced Methods
- [ ] Autoencoder anomaly detection êµ¬í˜„
- [ ] Ensemble methods (XGBoost + Autoencoder)
- [ ] Final report ì‘ì„±

---

## 5ï¸âƒ£ ë¦¬ìŠ¤í¬ ë° ì œì•½ì‚¬í•­

### ğŸ”´ Critical Risks
1. **ì–‘í’ˆ ìƒ˜í”Œ ë¶€ì¡±**:
   - íŒŒì¼: 13ê°œ (100W 1ê°œ, 200W 1ê°œÃ—9íŒŒì¼ + ì†ŒìŒ ì¬ë¶„ë¥˜)
   - ì˜í–¥: Val/Test ì‹ ë¢°ì„± ë‚®ìŒ, ì¼ë°˜í™” ì„±ëŠ¥ ê²€ì¦ ì–´ë ¤ì›€
   - ì™„í™”: Cross-validation í•„ìˆ˜, ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ì¥ê¸° ê³„íš

2. **ë‚®ì€ í´ë˜ìŠ¤ ë¶„ë¦¬ ëŠ¥ë ¥**:
   - Large effect size feature 0ê°œ
   - ì˜í–¥: ë†’ì€ ë¶„ë¥˜ ì •í™•ë„ ê¸°ëŒ€ ì–´ë ¤ì›€
   - ì™„í™”: ì£¼íŒŒìˆ˜ ì˜ì—­ feature ì¶”ê°€, ensemble ë°©ë²•

### ğŸŸ¡ Medium Risks
3. **Feature ì¤‘ë³µì„±**:
   - 31ê°œ ê³ ìƒê´€ ìŒ
   - ì˜í–¥: ëª¨ë¸ ë³µì¡ë„ ì¦ê°€, í•´ì„ ì–´ë ¤ì›€
   - ì™„í™”: Feature selection, PCA

4. **Val/Test ë¶ˆê· í˜•**:
   - Val ì •ìƒ 4ê°œ, Test ì •ìƒ 14ê°œ
   - ì˜í–¥: ê²€ì¦/í‰ê°€ ë¶ˆì•ˆì •
   - ì™„í™”: Cross-validation ì‚¬ìš©

### ğŸŸ¢ Low Risks
5. **ê²°ì¸¡ì¹˜**:
   - 5ê°œ ìœˆë„ìš° (0.7%)
   - ì˜í–¥: ë¯¸ë¯¸
   - ì™„í™”: ë‹¨ìˆœ ì œê±°

6. **Outliers**:
   - 40/49 featuresì— ì´ìƒì¹˜ ì¡´ì¬
   - ì˜í–¥: XGBoostëŠ” robust
   - ì™„í™”: í•„ìš” ì‹œ ëª¨ë¸ í•™ìŠµ í›„ ê²€í† 

---

## 6ï¸âƒ£ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­

### âœ… Phase 1 ì„±ê³¼
1. **í’ˆì§ˆ ê´€ë¦¬ ê°œì„ **: 51.7% â†’ 95.0% ì‚¬ìš© ê°€ëŠ¥ íŒŒì¼
2. **ë°ì´í„° ë¬´ê²°ì„± í™•ë³´**: ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦ 5ê°œ í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼
3. **Feature ì¶”ì¶œ ì™„ë£Œ**: 49ê°œ ì‹œê°„ ì˜ì—­ feature ìƒì„±
4. **ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ**: EDA ê²°ê³¼ ê¸°ë°˜ Phase 2 ë¡œë“œë§µ ìˆ˜ë¦½

### ğŸ¯ Phase 2 í•µì‹¬ ëª©í‘œ
1. **Baseline ì„±ëŠ¥ í™•ë¦½**: XGBoostë¡œ AUC > 0.80 ë‹¬ì„±
2. **Feature Engineering**: ì£¼íŒŒìˆ˜ ì˜ì—­ feature ì¶”ê°€ë¡œ ì„±ëŠ¥ ê°œì„ 
3. **ê²€ì¦ ì „ëµ ìˆ˜ë¦½**: Cross-validationìœ¼ë¡œ ì‹ ë¢°ì„± ìˆëŠ” í‰ê°€

### ğŸ“‹ ì¦‰ì‹œ ì‹¤í–‰ í•­ëª©
```python
# 1. ê²°ì¸¡ì¹˜ ì œê±°
features_clean = features.dropna()

# 2. Train/Val/Test split í™•ì¸
print(features_clean.groupby(['split_set', 'label_binary']).size())

# 3. XGBoost baseline í•™ìŠµ ì¤€ë¹„
X = features_clean[sensor_features]
y = features_clean['label_binary']
```

### ğŸš€ Next Steps
**ì´ì œ Phase 2ë¡œ ì§„í–‰ ì¤€ë¹„ ì™„ë£Œ!**

ê³„ì† ì§„í–‰í• ê¹Œìš”? XGBoost baseline ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?
