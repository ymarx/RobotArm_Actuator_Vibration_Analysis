# Phase 2 ì‹œì‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

**ëª©í‘œ**: XGBoost Baseline ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
**ì˜ˆìƒ ì†Œìš”**: 2-3 ì‹œê°„
**ì „ì œ ì¡°ê±´**: Phase 1 EDA ì™„ë£Œ âœ…

---

## âœ… Phase 1 ì™„ë£Œ í™•ì¸

- [x] ë°ì´í„° í’ˆì§ˆ ê¸°ì¤€ ì¡°ì • (95.0% ì‚¬ìš© ê°€ëŠ¥)
- [x] ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦ (5ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼)
- [x] Feature ì¶”ì¶œ (49ê°œ ì‹œê°„ ì˜ì—­)
- [x] EDA ë¶„ì„ ì™„ë£Œ
- [x] Phase 2 ì¤€ë¹„ì‚¬í•­ ì •ë¦¬

---

## ğŸ¯ Phase 2 ëª©í‘œ

### ì£¼ìš” ëª©í‘œ
1. **XGBoost Baseline ì„±ëŠ¥ í™•ë¦½**: AUC-ROC > 0.80
2. **Feature Importance íŒŒì•…**: ìƒìœ„ 20ê°œ ì¤‘ìš” feature ì„ ì •
3. **ê²€ì¦ ì „ëµ ìˆ˜ë¦½**: 5-Fold CVë¡œ ì‹ ë¢°ì„± ìˆëŠ” í‰ê°€

### ì„±ê³µ ê¸°ì¤€
- âœ… Train AUC > 0.85
- âœ… CV Mean AUC > 0.75 (Â±0.05)
- âœ… Test AUC > 0.70
- âœ… Recall (ë¶ˆëŸ‰ íƒì§€ìœ¨) > 0.80
- âœ… Precision > 0.85

---

## ğŸ“‹ ì‹¤í–‰ ë‹¨ê³„

### Step 1: ë°ì´í„° ì¤€ë¹„ (10ë¶„)

```python
import pandas as pd
import numpy as np
from pathlib import Path

# ë°ì´í„° ë¡œë“œ
features = pd.read_parquet('data/processed/features_combined_v1.parquet')

# ê²°ì¸¡ì¹˜ ì œê±° (5ê°œ ìœˆë„ìš°)
features_clean = features.dropna()
print(f"After removing NaN: {len(features_clean)} windows")

# Train/Val/Test ë¶„ë¦¬
train_df = features_clean[features_clean['split_set'] == 'train']
val_df = features_clean[features_clean['split_set'] == 'val']
test_df = features_clean[features_clean['split_set'] == 'test']

# Feature columns
sensor_features = [col for col in features_clean.columns
                   if col.startswith(('acc_', 'Gyro_'))]

# X, y ì¤€ë¹„
X_train = train_df[sensor_features]
y_train = train_df['label_binary']
X_val = val_df[sensor_features]
y_val = val_df['label_binary']
X_test = test_df[sensor_features]
y_test = test_df['label_binary']

print(f"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
print(f"Normal/Abnormal - Train: {y_train.sum()}/{len(y_train)-y_train.sum()}")
```

**ì˜ˆìƒ ê²°ê³¼**:
```
After removing NaN: 679 windows
Train: 476, Val: 92, Test: 111
Normal/Abnormal - Train: 160/316
```

---

### Step 2: XGBoost Baseline (30ë¶„)

```python
import xgboost as xgb
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix

# íŒŒë¼ë¯¸í„° ì„¤ì •
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'scale_pos_weight': (len(y_train) - y_train.sum()) / y_train.sum(),  # ë¶ˆëŸ‰/ì •ìƒ
    'max_depth': 6,
    'learning_rate': 0.01,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42
}

# DMatrix ìƒì„±
dtrain = xgb.DMatrix(X_train, label=y_train)
dval = xgb.DMatrix(X_val, label=y_val)

# í•™ìŠµ
evals = [(dtrain, 'train'), (dval, 'val')]
model = xgb.train(
    params,
    dtrain,
    num_boost_round=1000,
    evals=evals,
    early_stopping_rounds=50,
    verbose_eval=50
)

# ì˜ˆì¸¡
y_pred_proba = model.predict(dval)
y_pred = (y_pred_proba > 0.5).astype(int)

# í‰ê°€
print(f"Validation AUC: {roc_auc_score(y_val, y_pred_proba):.4f}")
print("\nClassification Report:")
print(classification_report(y_val, y_pred, target_names=['Abnormal', 'Normal']))
```

**ì²´í¬í¬ì¸íŠ¸**: Val AUC > 0.70 ë‹¬ì„± í™•ì¸

---

### Step 3: Cross-Validation (40ë¶„)

```python
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score

# 5-Fold CV
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = []

for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):
    print(f"\n{'='*50}")
    print(f"Fold {fold+1}/5")
    print(f"{'='*50}")

    X_tr, X_vl = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_vl = y_train.iloc[train_idx], y_train.iloc[val_idx]

    dtrain = xgb.DMatrix(X_tr, label=y_tr)
    dval = xgb.DMatrix(X_vl, label=y_vl)

    model = xgb.train(
        params,
        dtrain,
        num_boost_round=1000,
        evals=[(dtrain, 'train'), (dval, 'val')],
        early_stopping_rounds=50,
        verbose_eval=False
    )

    y_pred = model.predict(dval)
    auc = roc_auc_score(y_vl, y_pred)
    cv_scores.append(auc)
    print(f"Fold {fold+1} AUC: {auc:.4f}")

print(f"\n{'='*50}")
print(f"Cross-Validation Results:")
print(f"{'='*50}")
print(f"Mean AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")
print(f"Scores: {[f'{s:.4f}' for s in cv_scores]}")
```

**ì²´í¬í¬ì¸íŠ¸**: Mean AUC > 0.75 ë‹¬ì„± í™•ì¸

---

### Step 4: Feature Importance (20ë¶„)

```python
# Feature importance ì¶”ì¶œ
importance = model.get_score(importance_type='gain')
importance_df = pd.DataFrame([
    {'feature': k, 'importance': v}
    for k, v in importance.items()
]).sort_values('importance', ascending=False)

# Top 20 features
print("\nTop 20 Important Features:")
print(importance_df.head(20).to_string(index=False))

# ì €ì¥
importance_df.to_csv('claudedocs/feature_importance_baseline.csv', index=False)

# ì¤‘ìš”ë„ ëˆ„ì  ë¶„í¬
importance_df['cumsum'] = importance_df['importance'].cumsum() / importance_df['importance'].sum()
top_n_for_80pct = len(importance_df[importance_df['cumsum'] <= 0.8])
print(f"\n80% ì„¤ëª…ë ¥ì„ ìœ„í•œ feature ìˆ˜: {top_n_for_80pct}")
```

**ì²´í¬í¬ì¸íŠ¸**: ìƒìœ„ 20ê°œ featureê°€ ì „ì²´ì˜ 80% ì´ìƒ ì„¤ëª…ë ¥ í™•ì¸

---

### Step 5: Test Set í‰ê°€ (20ë¶„)

```python
# Test set ì˜ˆì¸¡
dtest = xgb.DMatrix(X_test, label=y_test)
y_test_pred_proba = model.predict(dtest)
y_test_pred = (y_test_pred_proba > 0.5).astype(int)

# í‰ê°€
test_auc = roc_auc_score(y_test, y_test_pred_proba)
print(f"\n{'='*50}")
print(f"Test Set Evaluation")
print(f"{'='*50}")
print(f"Test AUC: {test_auc:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_test_pred, target_names=['Abnormal', 'Normal']))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))

# ì €ì¥
results = {
    'val_auc': roc_auc_score(y_val, y_pred_proba),
    'cv_mean_auc': np.mean(cv_scores),
    'cv_std_auc': np.std(cv_scores),
    'test_auc': test_auc
}
pd.DataFrame([results]).to_csv('claudedocs/baseline_results.csv', index=False)
```

**ì²´í¬í¬ì¸íŠ¸**: Test AUC > 0.70 ë‹¬ì„± í™•ì¸

---

### Step 6: ê²°ê³¼ ì •ë¦¬ ë° ë³´ê³ ì„œ (30ë¶„)

```python
# ì¢…í•© ì„±ëŠ¥ ìš”ì•½
summary = f"""
XGBoost Baseline Model Performance
==================================

Data:
- Train: {len(X_train)} windows (Normal: {y_train.sum()}, Abnormal: {len(y_train)-y_train.sum()})
- Val:   {len(X_val)} windows (Normal: {y_val.sum()}, Abnormal: {len(y_val)-y_val.sum()})
- Test:  {len(X_test)} windows (Normal: {y_test.sum()}, Abnormal: {len(y_test)-y_test.sum()})

Model Parameters:
- max_depth: {params['max_depth']}
- learning_rate: {params['learning_rate']}
- scale_pos_weight: {params['scale_pos_weight']:.2f}

Results:
- Validation AUC:  {results['val_auc']:.4f}
- CV Mean AUC:     {results['cv_mean_auc']:.4f} Â± {results['cv_std_auc']:.4f}
- Test AUC:        {results['test_auc']:.4f}

Top 5 Features:
{importance_df.head(5).to_string(index=False)}

Next Steps:
1. Feature Engineering (ì£¼íŒŒìˆ˜ ì˜ì—­)
2. Hyperparameter Tuning
3. Threshold Optimization
"""

print(summary)
with open('claudedocs/baseline_summary.txt', 'w') as f:
    f.write(summary)
```

---

## ğŸš¨ ì£¼ì˜ì‚¬í•­

### ë°ì´í„° ì´ìŠˆ
1. **Val/Test ë¶ˆê· í˜•**: ì •ìƒ ìƒ˜í”Œ ë§¤ìš° ë¶€ì¡± (Val: 4ê°œ, Test: 14ê°œ)
   - â†’ CV ê²°ê³¼ë¥¼ ë” ì‹ ë¢°
   - â†’ Test ê²°ê³¼ëŠ” ì°¸ê³ ìš©

2. **í´ë˜ìŠ¤ ë¶ˆê· í˜•**: ë¶ˆëŸ‰:ì •ìƒ = 2:1 (train)
   - â†’ `scale_pos_weight` ì„¤ì • í•„ìˆ˜
   - â†’ Precision/Recall ê· í˜• ê³ ë ¤

### ì„±ëŠ¥ ê¸°ëŒ€ì¹˜
- **í˜„ì‹¤ì  ëª©í‘œ**: Test AUC 0.70-0.80
- **ì´ìœ **: Large effect size feature ì—†ìŒ (Cohen's d < 0.8)
- **ê°œì„  ë°©í–¥**: ì£¼íŒŒìˆ˜ ì˜ì—­ feature ì¶”ê°€ ì‹œ 0.80-0.85 ê¸°ëŒ€

### ì‹œê°„ ë°°ë¶„
- ì´ ì˜ˆìƒ ì‹œê°„: 2.5ì‹œê°„
- Step 1-2 (Baseline): 40ë¶„
- Step 3 (CV): 40ë¶„
- Step 4-5 (Importance + Test): 40ë¶„
- Step 6 (ì •ë¦¬): 30ë¶„

---

## ğŸ“ ìƒì„±ë  íŒŒì¼

```
claudedocs/
â”œâ”€â”€ feature_importance_baseline.csv
â”œâ”€â”€ baseline_results.csv
â””â”€â”€ baseline_summary.txt

models/ (optional)
â””â”€â”€ xgboost_baseline_v1.json
```

---

## âœ… ì™„ë£Œ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Baseline ëª¨ë¸ í•™ìŠµ ì™„ë£Œ
- [ ] CV Mean AUC > 0.75 ë‹¬ì„±
- [ ] Feature importance ë¶„ì„ ì™„ë£Œ
- [ ] Test set í‰ê°€ ì™„ë£Œ
- [ ] ê²°ê³¼ ë³´ê³ ì„œ ì‘ì„± ì™„ë£Œ
- [ ] ë‹¤ìŒ ë‹¨ê³„ (Feature Engineering) ê³„íš ìˆ˜ë¦½

---

**ì¤€ë¹„ ì™„ë£Œ! Phase 2ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?**
